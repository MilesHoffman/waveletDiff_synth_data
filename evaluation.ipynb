{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# WaveletDiff Evaluation\n",
                "\n",
                "This notebook evaluates a trained WaveletDiff model directly from your Google Colab environment using a checkpoint from Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import os\n",
                "\n",
                "# --- Drive Paths ---\n",
                "DRIVE_MOUNT_PATH = \"/content/drive\" # @param {type:\"string\"}\n",
                "CHECKPOINT_NAME = \"checkpoint.ckpt\" # @param {type:\"string\"}\n",
                "DRIVE_CHECKPOINT_FOLDER = \"/content/drive/MyDrive/personal_drive/trading/waveletDiff/checkpoints\" # @param {type:\"string\"}\n",
                "DRIVE_CHECKPOINT_PATH = os.path.join(DRIVE_CHECKPOINT_FOLDER, CHECKPOINT_NAME)\n",
                "\n",
                "# --- Repository Settings ---\n",
                "REPO_BRANCH = \"develop\" # @param {type:\"string\"}\n",
                "\n",
                "# --- Evaluation Settings ---\n",
                "DATASET = \"stocks\" # @param {type:\"string\"}\n",
                "EXPERIMENT_NAME = \"evaluation_run\" # @param {type:\"string\"}\n",
                "NUM_SAMPLES = 2000 # @param {type:\"integer\"}\n",
                "SAMPLING_METHOD = \"ddpm\" # @param [\"ddpm\", \"ddim\"]\n",
                "COMPILE_MODE = \"none\" # @param [\"none\", \"default\", \"reduce-overhead\", \"max-autotune\"]\n",
                "DEVICE = \"cuda\" # @param [\"cuda\", \"cpu\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Setup (Clone, Install, Mount)\n",
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "from google.colab import drive\n",
                "\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_NAME = \"waveletDiff_synth_data\"\n",
                "REPO_PATH = os.path.abspath(REPO_NAME)\n",
                "\n",
                "# 1. Clone or Pull Repo\n",
                "if os.path.exists(REPO_PATH):\n",
                "    # Check for nested directory issue (common in Colab if run multiple times)\n",
                "    nested_path = os.path.join(REPO_PATH, REPO_NAME)\n",
                "    if os.path.exists(nested_path):\n",
                "        print(f\"⚠️ Detected nested repository at {nested_path}. cleaning up...\")\n",
                "        shutil.rmtree(REPO_PATH)\n",
                "        print(f\"Cloning {REPO_URL}...\")\n",
                "        !git clone {REPO_URL} {REPO_NAME}\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "    else:\n",
                "        print(f\"Pulling latest for {REPO_NAME}...\")\n",
                "        !git -C {REPO_NAME} fetch origin\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "        !git -C {REPO_NAME} pull origin {REPO_BRANCH}\n",
                "else:\n",
                "    print(f\"Cloning {REPO_URL}...\")\n",
                "    !git clone {REPO_URL} {REPO_NAME}\n",
                "    !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "\n",
                "# 2. Install Dependencies (using scikit-learn instead of sklearn)\n",
                "!pip install -q pytorch-lightning pywavelets scipy pandas tqdm scikit-learn tslearn seaborn\n",
                "\n",
                "# 3. Mount Google Drive\n",
                "if not os.path.exists(DRIVE_MOUNT_PATH):\n",
                "    drive.mount(DRIVE_MOUNT_PATH)\n",
                "\n",
                "# 4. Setup Paths\n",
                "if os.path.join(REPO_PATH, \"src\") not in sys.path:\n",
                "    sys.path.append(os.path.join(REPO_PATH, \"src\"))\n",
                "if os.path.join(REPO_PATH, \"src\", \"evaluation\") not in sys.path:\n",
                "    sys.path.append(os.path.join(REPO_PATH, \"src\", \"evaluation\"))\n",
                "\n",
                "# 5. Prepare Checkpoint and Configs\n",
                "local_exp_dir = os.path.join(REPO_PATH, \"outputs\", EXPERIMENT_NAME)\n",
                "os.makedirs(local_exp_dir, exist_ok=True)\n",
                "\n",
                "if os.path.exists(DRIVE_CHECKPOINT_PATH):\n",
                "    dst_ckpt = os.path.join(local_exp_dir, \"checkpoint.ckpt\")\n",
                "    shutil.copy2(DRIVE_CHECKPOINT_PATH, dst_ckpt)\n",
                "    print(f\"✅ Copied checkpoint to {dst_ckpt}\")\n",
                "else:\n",
                "    print(f\"❌ ERROR: Checkpoint not found at {DRIVE_CHECKPOINT_PATH}\")\n",
                "\n",
                "# Sync configs to repo root so scripts can find them at ../configs\n",
                "configs_src = os.path.join(REPO_PATH, \"WaveletDiff_source\", \"configs\")\n",
                "configs_dst = os.path.join(REPO_PATH, \"configs\")\n",
                "\n",
                "if os.path.exists(configs_src):\n",
                "    if os.path.exists(configs_dst):\n",
                "        shutil.rmtree(configs_dst)\n",
                "    shutil.copytree(configs_src, configs_dst)\n",
                "    print(\"✅ Configs synced to repo root\")\n",
                "\n",
                "print(\"✅ Setup Complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gen_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Generate Samples\n",
                "import os\n",
                "os.chdir(os.path.join(REPO_PATH, \"src\"))\n",
                "\n",
                "print(f\"Generating {NUM_SAMPLES} samples using {SAMPLING_METHOD} with compile_mode={COMPILE_MODE}...\")\n",
                "\n",
                "status = !python sample.py \\\n",
                "    --experiment_name {EXPERIMENT_NAME} \\\n",
                "    --dataset {DATASET} \\\n",
                "    --num_samples {NUM_SAMPLES} \\\n",
                "    --sampling_method {SAMPLING_METHOD} \\\n",
                "    --compile_mode {COMPILE_MODE}\n",
                "\n",
                "print(\"\\n\".join(status))\n",
                "\n",
                "if any(\"Traceback\" in s or \"Error\" in s for s in status):\n",
                "    print(\"\\n❌ Generation failed. Check the error log above.\")\n",
                "else:\n",
                "    print(\"\\n✅ Sampling Complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d600f08e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Imports and Setup\n",
                "import sys\n",
                "import numpy as np\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.decomposition import PCA\n",
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "from discriminative_metrics import discriminative_score_metrics\n",
                "from predictive_metrics import predictive_score_metrics\n",
                "from context_fid import Context_FID\n",
                "from cross_correlation import CrossCorrelLoss\n",
                "from metric_utils import display_scores\n",
                "from dtw import dtw_js_divergence_distance\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_context(\"notebook\", font_scale=1.2)\n",
                "COLORS = {\"Real\": \"#d62728\", \"Generated\": \"#1f77b4\"} # Red for Real, Blue for Generated"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b5f4a164",
            "metadata": {},
            "source": [
                "### Load Real and Generated Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "96588f6e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths are relative to 'src' directory where we currently are\n",
                "real_data_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples.npy\"\n",
                "gen_data_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples.npy\"\n",
                "\n",
                "if not os.path.exists(real_data_path):\n",
                "    raise FileNotFoundError(f\"Could not find real samples at {os.path.abspath(real_data_path)}. Check Cell 3 output.\")\n",
                "\n",
                "real_data = np.load(real_data_path)\n",
                "generated_data = np.load(gen_data_path)\n",
                "print(f\"Loaded Real: {real_data.shape}, Generated: {generated_data.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6fb459a",
            "metadata": {},
            "outputs": [],
            "source": [
                "num_samples = min(real_data.shape[0], generated_data.shape[0])\n",
                "if real_data.shape[0] > num_samples:\n",
                "    print(f\"WARNING: Using all {num_samples} generated samples for evaluation.\")\n",
                "else:\n",
                "    print(f\"Number of samples: {num_samples}\")\n",
                "\n",
                "random_indices = np.random.choice(len(real_data), num_samples, replace=False)\n",
                "real_data = real_data[random_indices]\n",
                "random_indices = np.random.choice(len(generated_data), num_samples, replace=False)\n",
                "generated_data = generated_data[random_indices]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71296ef8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# minmax scale the inputs for fair comparison\n",
                "data_min = np.min(real_data, axis=(0,1), keepdims=True)\n",
                "data_max = np.max(real_data, axis=(0,1), keepdims=True)\n",
                "\n",
                "real_data = (real_data - data_min) / (data_max - data_min)\n",
                "generated_data = (generated_data - data_min) / (data_max - data_min)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vis_md",
            "metadata": {},
            "source": [
                "### Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_tsne",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title t-SNE & PCA Visualization\n",
                "\n",
                "def plot_distribution_reduction(real, generated, n_samples=1000):\n",
                "    # Flatten time series for t-SNE (Standard approach in TimeGAN/Diffusion-TS)\n",
                "    # Shape: (N, T*D)\n",
                "    n_samples = min(n_samples, len(real), len(generated))\n",
                "    \n",
                "    real_flat = real[:n_samples].reshape(n_samples, -1)\n",
                "    gen_flat = generated[:n_samples].reshape(n_samples, -1)\n",
                "    \n",
                "    # Concatenate\n",
                "    data = np.concatenate([real_flat, gen_flat], axis=0)\n",
                "    labels = [\"Real\"] * n_samples + [\"Generated\"] * n_samples\n",
                "    \n",
                "    # t-SNE\n",
                "    print(\"Running t-SNE...\")\n",
                "    tsne = TSNE(n_components=2, perplexity=40, n_iter=300)\n",
                "    tsne_results = tsne.fit_transform(data)\n",
                "    \n",
                "    # Plot\n",
                "    plt.figure(figsize=(16, 6))\n",
                "    \n",
                "    # t-SNE Plot\n",
                "    plt.subplot(1, 2, 1)\n",
                "    sns.scatterplot(x=tsne_results[:n_samples, 0], y=tsne_results[:n_samples, 1], \n",
                "                    color=COLORS[\"Real\"], alpha=0.3, label=\"Real\", s=20)\n",
                "    sns.scatterplot(x=tsne_results[n_samples:, 0], y=tsne_results[n_samples:, 1],\n",
                "                    color=COLORS[\"Generated\"], alpha=0.3, label=\"Generated\", s=20)\n",
                "    plt.title(\"t-SNE Visualization\")\n",
                "    plt.legend()\n",
                "    \n",
                "    # PCA Plot (for variance check)\n",
                "    print(\"Running PCA...\")\n",
                "    pca = PCA(n_components=2)\n",
                "    pca_results = pca.fit_transform(data)\n",
                "    \n",
                "    plt.subplot(1, 2, 2)\n",
                "    sns.scatterplot(x=pca_results[:n_samples, 0], y=pca_results[:n_samples, 1], \n",
                "                    color=COLORS[\"Real\"], alpha=0.3, label=\"Real\", s=20)\n",
                "    sns.scatterplot(x=pca_results[n_samples:, 0], y=pca_results[n_samples:, 1],\n",
                "                    color=COLORS[\"Generated\"], alpha=0.3, label=\"Generated\", s=20)\n",
                "    plt.title(\"PCA Visualization\")\n",
                "    plt.legend()\n",
                "    \n",
                "    plt.show()\n",
                "\n",
                "plot_distribution_reduction(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_pdf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Probability Density Function (Data Values)\n",
                "\n",
                "def plot_pdf(real, generated):\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    # Flatten all data to compare value distributions\n",
                "    sns.kdeplot(real.flatten(), fill=True, color=COLORS[\"Real\"], label=\"Real\", alpha=0.3)\n",
                "    sns.kdeplot(generated.flatten(), fill=True, color=COLORS[\"Generated\"], label=\"Generated\", alpha=0.3)\n",
                "    \n",
                "    plt.title(\"Probability Density Function (All Values)\")\n",
                "    plt.xlabel(\"Data Value\")\n",
                "    plt.ylabel(\"Density\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "plot_pdf(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_samples",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Sample Visualization (Generated vs Real)\n",
                "\n",
                "def plot_samples(real, generated, n_samples=5):\n",
                "    # Assuming shape (N, T, D)\n",
                "    # We exclude the last feature if it is volume (assuming > 1 feature)\n",
                "    n_features = real.shape[2]\n",
                "    plot_features = n_features - 1 if n_features > 1 else n_features\n",
                "    \n",
                "    fig, axes = plt.subplots(2, n_samples, figsize=(n_samples * 4, 6), sharey=True)\n",
                "    \n",
                "    for i in range(n_samples):\n",
                "        # Real Samples\n",
                "        for f in range(plot_features):\n",
                "            axes[0, i].plot(real[i, :, f], alpha=0.8)\n",
                "        axes[0, i].set_title(f\"Real Sample {i}\")\n",
                "        if i == 0: axes[0, i].set_ylabel(\"Value (MinMax Scaled)\")\n",
                "        \n",
                "        # Generated Samples\n",
                "        for f in range(plot_features):\n",
                "            axes[1, i].plot(generated[i, :, f], alpha=0.8)\n",
                "        axes[1, i].set_title(f\"Gen Sample {i}\")\n",
                "        if i == 0: axes[1, i].set_ylabel(\"Value (MinMax Scaled)\")\n",
                "    \n",
                "    # Create a dummy legend\n",
                "    from matplotlib.lines import Line2D\n",
                "    lines = [Line2D([0], [0], color=f\"C{i}\", lw=2) for i in range(plot_features)]\n",
                "    fig.legend(lines, [f\"Feature {i}\" for i in range(plot_features)], loc='lower center', ncol=plot_features)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    print(f\"Note: Showing {plot_features} features (excluding last/volume feature if D>1)\")\n",
                "    plt.show()\n",
                "\n",
                "plot_samples(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics_md",
            "metadata": {},
            "source": [
                "### Statistical Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "stat_metrics",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Statistical Distribution Metrics (Real vs Generated Comparison)\n",
                "from scipy.stats import skew, kurtosis\n",
                "\n",
                "def calculate_statistical_metrics(real, generated):\n",
                "    \"\"\"Calculate and display comprehensive statistical metrics.\"\"\"\n",
                "    # Flatten over samples and time to get distribution of values per feature\n",
                "    r_flat = real.reshape(-1, real.shape[2])\n",
                "    g_flat = generated.reshape(-1, generated.shape[2])\n",
                "    n_features = real.shape[2]\n",
                "    \n",
                "    # Calculate per-feature statistics\n",
                "    real_stats = {\n",
                "        \"Mean\": np.mean(r_flat, axis=0),\n",
                "        \"Std\": np.std(r_flat, axis=0),\n",
                "        \"Skewness\": skew(r_flat, axis=0),\n",
                "        \"Kurtosis\": kurtosis(r_flat, axis=0),\n",
                "        \"Min\": np.min(r_flat, axis=0),\n",
                "        \"Max\": np.max(r_flat, axis=0),\n",
                "    }\n",
                "    \n",
                "    gen_stats = {\n",
                "        \"Mean\": np.mean(g_flat, axis=0),\n",
                "        \"Std\": np.std(g_flat, axis=0),\n",
                "        \"Skewness\": skew(g_flat, axis=0),\n",
                "        \"Kurtosis\": kurtosis(g_flat, axis=0),\n",
                "        \"Min\": np.min(g_flat, axis=0),\n",
                "        \"Max\": np.max(g_flat, axis=0),\n",
                "    }\n",
                "    \n",
                "    # Print comparison table\n",
                "    print(\"=\" * 80)\n",
                "    print(\"STATISTICAL COMPARISON: Real vs Generated Data\")\n",
                "    print(\"=\" * 80)\n",
                "    \n",
                "    # Per-feature comparison\n",
                "    for f in range(n_features):\n",
                "        print(f\"\\n--- Feature {f} ---\")\n",
                "        print(f\"{'Metric':<15} {'Real':>12} {'Generated':>12} {'Diff (Abs)':>12}\")\n",
                "        print(\"-\" * 55)\n",
                "        for stat_name in real_stats:\n",
                "            r_val = real_stats[stat_name][f]\n",
                "            g_val = gen_stats[stat_name][f]\n",
                "            diff = abs(r_val - g_val)\n",
                "            print(f\"{stat_name:<15} {r_val:>12.4f} {g_val:>12.4f} {diff:>12.4f}\")\n",
                "    \n",
                "    # Aggregate metrics (averaged across features)\n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(\"AGGREGATE METRICS (Averaged Across All Features)\")\n",
                "    print(\"=\" * 80)\n",
                "    \n",
                "    aggregate = {\n",
                "        \"Mean MAE\": np.mean(np.abs(real_stats[\"Mean\"] - gen_stats[\"Mean\"])),\n",
                "        \"Std MAE\": np.mean(np.abs(real_stats[\"Std\"] - gen_stats[\"Std\"])),\n",
                "        \"Skewness MAE\": np.mean(np.abs(real_stats[\"Skewness\"] - gen_stats[\"Skewness\"])),\n",
                "        \"Kurtosis MAE\": np.mean(np.abs(real_stats[\"Kurtosis\"] - gen_stats[\"Kurtosis\"])),\n",
                "    }\n",
                "    \n",
                "    print(f\"{'Metric':<20} {'Value':>12} {'Interpretation'}\")\n",
                "    print(\"-\" * 60)\n",
                "    for k, v in aggregate.items():\n",
                "        quality = \"✅ Good\" if v < 0.05 else (\"⚠️ Moderate\" if v < 0.15 else \"❌ High\")\n",
                "        print(f\"{k:<20} {v:>12.6f} {quality}\")\n",
                "    \n",
                "    return aggregate\n",
                "\n",
                "stat_results = calculate_statistical_metrics(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "da09d10a",
            "metadata": {},
            "source": [
                "### Discriminative Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5b47b22",
            "metadata": {},
            "outputs": [],
            "source": [
                "iterations = 5\n",
                "discriminative_score = []\n",
                "\n",
                "for i in range(iterations):\n",
                "    temp_disc, fake_acc, real_acc = discriminative_score_metrics(real_data, generated_data)\n",
                "    discriminative_score.append(temp_disc)\n",
                "    print(f'Iter {i}: ', temp_disc, '\\n')\n",
                "      \n",
                "display_scores(discriminative_score)\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32ec7287",
            "metadata": {},
            "source": [
                "### Predictive Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d766f30c",
            "metadata": {},
            "outputs": [],
            "source": [
                "iterations = 5\n",
                "predictive_score = []\n",
                "for i in range(iterations):\n",
                "    temp_pred = predictive_score_metrics(real_data, generated_data)\n",
                "    predictive_score.append(temp_pred)\n",
                "    print(i, ' epoch: ', temp_pred, '\\n')\n",
                "      \n",
                "display_scores(predictive_score)\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18a2b417",
            "metadata": {},
            "source": [
                "### Context-FID Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2bdc9d5e",
            "metadata": {},
            "outputs": [],
            "source": [
                "context_fid_score = []\n",
                "\n",
                "for i in range(iterations):\n",
                "    context_fid = Context_FID(real_data, generated_data)\n",
                "    context_fid_score.append(context_fid)\n",
                "    print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
                "      \n",
                "display_scores(context_fid_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8ea6fe97",
            "metadata": {},
            "source": [
                "### Correlational Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64dba75d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def random_choice(size, num_select=100):\n",
                "    select_idx = np.random.randint(low=0, high=size, size=(num_select,))\n",
                "    return select_idx\n",
                "\n",
                "x_real = torch.from_numpy(real_data)\n",
                "x_fake = torch.from_numpy(generated_data)\n",
                "\n",
                "correlational_score = []\n",
                "size = 1000\n",
                "\n",
                "for i in range(iterations):\n",
                "    real_idx = random_choice(x_real.shape[0], size)\n",
                "    fake_idx = random_choice(x_fake.shape[0], size)\n",
                "    corr = CrossCorrelLoss(x_real[real_idx, :, :], name='CrossCorrelLoss')\n",
                "    loss = corr.compute(x_fake[fake_idx, :, :])\n",
                "    correlational_score.append(loss.item())\n",
                "    print(f'Iter {i}: ', 'cross-correlation =', loss.item(), '\\n')\n",
                "\n",
                "display_scores(correlational_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4237eebe",
            "metadata": {},
            "source": [
                "### DTW distance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cc548e60",
            "metadata": {},
            "outputs": [],
            "source": [
                "iterations = 5\n",
                "js_results = []\n",
                "for i in range(iterations):\n",
                "    # Standard n_samples for DTW comparison is 100 (slow metric)\n",
                "    js_dist = dtw_js_divergence_distance(real_data, generated_data, n_samples=100)['js_divergence']\n",
                "    print(\"js_dist: \", round(js_dist, 4))\n",
                "    js_results.append(js_dist)\n",
                "display_scores(js_results)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# WaveletDiff Evaluation (Refactored)\n",
                "\n",
                "This notebook evaluates a trained WaveletDiff model. It acts as a frontend interface, delegating heavy logic to `src/evaluation` modules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import os\n",
                "\n",
                "# --- Drive Paths ---\n",
                "DRIVE_MOUNT_PATH = \"/content/drive\" # @param {type:\"string\"}\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/personal_drive/trading/waveletDiff\" # @param {type:\"string\"}\n",
                "CHECKPOINT_FOLDER = \"checkpoints\" # @param {type:\"string\"}\n",
                "SAMPLES_FOLDER = \"samples\" # @param {type:\"string\"}\n",
                "\n",
                "MODEL_FILENAME = \"stocks_experiment.tar.gz\" # @param {type:\"string\"}\n",
                "MODEL_BASENAME = MODEL_FILENAME.replace('.tar.gz', '').replace('.zip', '').replace('.ckpt', '').replace('.tgz', '').replace('.gz', '')\n",
                "\n",
                "DRIVE_CHECKPOINT_PATH = os.path.join(DRIVE_BASE_PATH, CHECKPOINT_FOLDER, MODEL_FILENAME)\n",
                "DRIVE_SAMPLES_PATH = os.path.join(DRIVE_BASE_PATH, SAMPLES_FOLDER, MODEL_BASENAME)\n",
                "\n",
                "# --- Repository Settings ---\n",
                "REPO_BRANCH = \"develop\" # @param {type:\"string\"}\n",
                "\n",
                "# --- Evaluation Settings ---\n",
                "DATASET = \"stocks\" # @param {type:\"string\"}\n",
                "EXPERIMENT_NAME = \"evaluation_run\" # @param {type:\"string\"}\n",
                "NUM_SAMPLES = 2000 # @param {type:\"integer\"}\n",
                "SAMPLING_METHOD = \"ddpm\" # @param [\"ddpm\", \"ddim\"]\n",
                "COMPILE_MODE = \"none\" # @param [\"none\", \"default\", \"reduce-overhead\", \"max-autotune\"]\n",
                "DEVICE = \"cuda\" # @param [\"cuda\", \"cpu\"]\n",
                "\n",
                "# --- Evaluation Options ---\n",
                "EXCLUDE_VOLUME = True # @param {type:\"boolean\"}\n",
                "CACHE_SAMPLES_TO_DRIVE = True # @param {type:\"boolean\"}\n",
                "USE_CACHED_SAMPLES = True # @param {type:\"boolean\"}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Setup (Clone, Install, Mount)\n",
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "from google.colab import drive\n",
                "\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_NAME = \"waveletDiff_synth_data\"\n",
                "REPO_PATH = os.path.abspath(REPO_NAME)\n",
                "\n",
                "# 1. Clone or Pull Repo\n",
                "if os.path.exists(REPO_PATH):\n",
                "    nested_path = os.path.join(REPO_PATH, REPO_NAME)\n",
                "    if os.path.exists(nested_path):\n",
                "        shutil.rmtree(REPO_PATH)\n",
                "        !git clone {REPO_URL} {REPO_NAME}\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "    else:\n",
                "        !git -C {REPO_NAME} fetch origin\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "        !git -C {REPO_NAME} pull origin {REPO_BRANCH}\n",
                "else:\n",
                "    !git clone {REPO_URL} {REPO_NAME}\n",
                "    !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "\n",
                "# 2. Install Dependencies\n",
                "!pip install -q pytorch-lightning pywavelets scipy pandas tqdm scikit-learn tslearn seaborn statsmodels\n",
                "\n",
                "# 3. Mount Drive & Setup Paths\n",
                "if not os.path.exists(DRIVE_MOUNT_PATH):\n",
                "    drive.mount(DRIVE_MOUNT_PATH)\n",
                "\n",
                "for p in [os.path.join(REPO_PATH, \"src\"), os.path.join(REPO_PATH, \"src\", \"evaluation\")]:\n",
                "    if p not in sys.path: sys.path.append(p)\n",
                "\n",
                "# 4. Prepare Experiments\n",
                "local_exp_dir = os.path.join(REPO_PATH, \"outputs\", EXPERIMENT_NAME)\n",
                "os.makedirs(local_exp_dir, exist_ok=True)\n",
                "\n",
                "if os.path.exists(DRIVE_CHECKPOINT_PATH):\n",
                "    print(f\"Unpacking model from {DRIVE_CHECKPOINT_PATH}...\")\n",
                "    if DRIVE_CHECKPOINT_PATH.endswith(\".ckpt\"):\n",
                "        shutil.copy2(DRIVE_CHECKPOINT_PATH, os.path.join(local_exp_dir, \"checkpoint.ckpt\"))\n",
                "    else:\n",
                "        shutil.unpack_archive(DRIVE_CHECKPOINT_PATH, local_exp_dir, format='gztar' if '.gz' in DRIVE_CHECKPOINT_PATH and not '.tar' in DRIVE_CHECKPOINT_PATH else None)\n",
                "else:\n",
                "    print(f\"❌ Model file not found.\")\n",
                "\n",
                "# Sync configs\n",
                "if os.path.exists(os.path.join(REPO_PATH, \"WaveletDiff_source\", \"configs\")):\n",
                "    shutil.rmtree(os.path.join(REPO_PATH, \"configs\"), ignore_errors=True)\n",
                "    shutil.copytree(os.path.join(REPO_PATH, \"WaveletDiff_source\", \"configs\"), os.path.join(REPO_PATH, \"configs\"))\n",
                "\n",
                "print(\"✅ Setup Complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gen_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Generate or Load Samples\n",
                "import numpy as np\n",
                "os.chdir(os.path.join(REPO_PATH, \"src\"))\n",
                "\n",
                "# Dollar space paths\n",
                "local_gen_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples.npy\"\n",
                "local_real_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples.npy\"\n",
                "drive_gen_path = os.path.join(DRIVE_SAMPLES_PATH, f\"{SAMPLING_METHOD}_samples.npy\")\n",
                "drive_real_path = os.path.join(DRIVE_SAMPLES_PATH, \"real_samples.npy\")\n",
                "\n",
                "# Reparameterized (norm) space paths\n",
                "local_gen_norm_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples_norm.npy\"\n",
                "local_real_norm_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples_norm.npy\"\n",
                "drive_gen_norm_path = os.path.join(DRIVE_SAMPLES_PATH, f\"{SAMPLING_METHOD}_samples_norm.npy\")\n",
                "drive_real_norm_path = os.path.join(DRIVE_SAMPLES_PATH, \"real_samples_norm.npy\")\n",
                "\n",
                "if USE_CACHED_SAMPLES and os.path.exists(drive_gen_path):\n",
                "    print(\"Loading cached samples...\")\n",
                "    os.makedirs(os.path.dirname(local_gen_path), exist_ok=True)\n",
                "    shutil.copy2(drive_gen_path, local_gen_path)\n",
                "    shutil.copy2(drive_real_path, local_real_path)\n",
                "    # Copy norm files if they exist\n",
                "    if os.path.exists(drive_gen_norm_path):\n",
                "        shutil.copy2(drive_gen_norm_path, local_gen_norm_path)\n",
                "        shutil.copy2(drive_real_norm_path, local_real_norm_path)\n",
                "else:\n",
                "    print(f\"Generating {NUM_SAMPLES} samples...\")\n",
                "    output = !python sample.py --experiment_name {EXPERIMENT_NAME} --dataset {DATASET} --num_samples {NUM_SAMPLES} --sampling_method {SAMPLING_METHOD} --compile_mode {COMPILE_MODE}\n",
                "    print(\"\\n\".join(output))\n",
                "    \n",
                "    if not os.path.exists(local_gen_path):\n",
                "        print(\"\\n❌ Generation failed: Output file not produced.\")\n",
                "        print(\"Possible causes: Checkpoint not found, CUDA error, or config mismatch.\")\n",
                "        raise FileNotFoundError(f\"File does not exist: {local_gen_path}\")\n",
                "\n",
                "    if CACHE_SAMPLES_TO_DRIVE:\n",
                "        os.makedirs(DRIVE_SAMPLES_PATH, exist_ok=True)\n",
                "        shutil.copy2(local_gen_path, drive_gen_path)\n",
                "        shutil.copy2(local_real_path, drive_real_path)\n",
                "        # Cache norm files if they exist\n",
                "        if os.path.exists(local_gen_norm_path):\n",
                "            shutil.copy2(local_gen_norm_path, drive_gen_norm_path)\n",
                "            shutil.copy2(local_real_norm_path, drive_real_norm_path)\n",
                "\n",
                "print(\"✅ Samples Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Initialize Modules\n",
                "import sys\n",
                "import numpy as np\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# New Refactored Modules\n",
                "from evaluation import visualizations as viz\n",
                "from evaluation import statistics as stats\n",
                "from evaluation import reporting as report\n",
                "from evaluation import wrappers\n",
                "from training import inline_evaluation\n",
                "\n",
                "# Load Dollar Space Data\n",
                "real_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples.npy\"\n",
                "gen_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples.npy\"\n",
                "real_data_full = np.load(real_path)\n",
                "generated_data_full = np.load(gen_path)\n",
                "\n",
                "print(f\"Loaded Full OHLCV (Dollar): Real {real_data_full.shape}, Gen {generated_data_full.shape}\")\n",
                "\n",
                "# Load Reparameterized (Norm) Space Data\n",
                "real_norm_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples_norm.npy\"\n",
                "gen_norm_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples_norm.npy\"\n",
                "HAS_NORM_DATA = os.path.exists(real_norm_path) and os.path.exists(gen_norm_path)\n",
                "\n",
                "if HAS_NORM_DATA:\n",
                "    real_data_norm_full = np.load(real_norm_path)\n",
                "    generated_data_norm_full = np.load(gen_norm_path)\n",
                "    print(f\"Loaded Reparameterized: Real {real_data_norm_full.shape}, Gen {generated_data_norm_full.shape}\")\n",
                "else:\n",
                "    print(\"⚠️ Reparameterized data not found. Re-run sample.py with updated code to generate.\")\n",
                "    real_data_norm_full = None\n",
                "    generated_data_norm_full = None\n",
                "\n",
                "# Downsample for metrics (keep indices consistent)\n",
                "n_s = min(2000, len(real_data_full), len(generated_data_full))\n",
                "np.random.seed(42)\n",
                "real_idx = np.random.choice(len(real_data_full), n_s, replace=False)\n",
                "gen_idx = np.random.choice(len(generated_data_full), n_s, replace=False)\n",
                "\n",
                "# Full OHLCV for sample visualizations (Dollar Space)\n",
                "real_data_ohlcv = real_data_full[real_idx]\n",
                "generated_data_ohlcv = generated_data_full[gen_idx]\n",
                "\n",
                "# Reparameterized subsets\n",
                "if HAS_NORM_DATA:\n",
                "    real_data_norm = real_data_norm_full[real_idx]\n",
                "    generated_data_norm = generated_data_norm_full[gen_idx]\n",
                "    \n",
                "    # Scale norm data for discriminative/predictive benchmarks\n",
                "    norm_min = np.min(real_data_norm, axis=(0,1), keepdims=True)\n",
                "    norm_max = np.max(real_data_norm, axis=(0,1), keepdims=True)\n",
                "    real_data_norm_scaled = (real_data_norm - norm_min) / (norm_max - norm_min + 1e-8)\n",
                "    generated_data_norm_scaled = (generated_data_norm - norm_min) / (norm_max - norm_min + 1e-8)\n",
                "\n",
                "# Apply EXCLUDE_VOLUME for metrics only\n",
                "if EXCLUDE_VOLUME and real_data_full.shape[2] > 1:\n",
                "    real_data = real_data_ohlcv[..., :-1]\n",
                "    generated_data = generated_data_ohlcv[..., :-1]\n",
                "else:\n",
                "    real_data = real_data_ohlcv\n",
                "    generated_data = generated_data_ohlcv\n",
                "\n",
                "# Prepare Raw vs Scaled (for metrics)\n",
                "real_data_raw, generated_data_raw = real_data.copy(), generated_data.copy()\n",
                "\n",
                "dmin, dmax = np.min(real_data, axis=(0,1), keepdims=True), np.max(real_data, axis=(0,1), keepdims=True)\n",
                "real_data_scaled = (real_data - dmin) / (dmax - dmin + 1e-8)\n",
                "generated_data_scaled = (generated_data - dmin) / (dmax - dmin + 1e-8)\n",
                "\n",
                "print(f\"Metrics Data (EXCLUDE_VOLUME={EXCLUDE_VOLUME}): {real_data.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vis_header",
            "metadata": {},
            "source": [
                "### Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Visual Analysis\n",
                "# t-SNE and PCA use scaled OHLC data (no volume)\n",
                "viz.plot_distribution_reduction(real_data_scaled, generated_data_scaled)\n",
                "viz.plot_pdf(real_data_scaled, generated_data_scaled)\n",
                "\n",
                "# Sample plots use FULL OHLCV data in Dollar Space\n",
                "viz.plot_candlesticks(real_data_ohlcv, generated_data_ohlcv)\n",
                "viz.plot_samples(real_data_ohlcv, generated_data_ohlcv)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics_header",
            "metadata": {},
            "source": [
                "### Comprehensive Metrics Evaluation\n",
                "\n",
                "Evaluates model performance across both Dollar Space and Reparameterized (Norm) Space.\n",
                "Includes Fidelity (Distribution), Utility (Predictive), and Diversity (Coverage/Memorization) metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dual_metrics_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Run Evaluation Metrics (Dual Space)\n",
                "from scipy.stats import wasserstein_distance\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "metrics_dict = {}\n",
                "\n",
                "# ==========================================\n",
                "# 1. DOLLAR SPACE EVALUATION\n",
                "# ==========================================\n",
                "if generated_data is not None:\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"EVALUATION: DOLLAR SPACE (Raw Prices)\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    # A. Statistical\n",
                "    stat_agg, real_stat_det, gen_stat_det = stats.calculate_statistical_metrics(real_data_raw, generated_data_raw)\n",
                "    metrics_dict['stat_results'] = stat_agg\n",
                "    \n",
                "    # B. Utility (TSTR vs TRTR)\n",
                "    print(\"\\nRunning Utility Checks (TSTR vs TRTR)...\")\n",
                "    # TSTR: Train on Synthetic, Test on Real\n",
                "    tstr_score = wrappers.run_predictive_benchmark(real_data, generated_data, iterations=1)[0]\n",
                "    # TRTR: Train on Real, Test on Real (Baseline)\n",
                "    trtr_score = wrappers.run_predictive_benchmark(real_data, real_data, iterations=1)[0]\n",
                "    \n",
                "    metrics_dict['predictive_score'] = [tstr_score]\n",
                "    # metrics_dict['utility_trtr'] = trtr_score\n",
                "    print(f\"  TSTR (Score): {tstr_score:.6f}\")\n",
                "    print(f\"  TRTR (Base):  {trtr_score:.6f}\")\n",
                "    print(f\"  Utility Gap:  {abs(tstr_score - trtr_score):.6f} (Lower is better)\")\n",
                "\n",
                "    # C. Discriminative Score\n",
                "    disc_scores = wrappers.run_discriminative_benchmark(real_data, generated_data, iterations=1)\n",
                "    metrics_dict['discriminative_score'] = disc_scores\n",
                "    \n",
                "    # D. Advanced Metrics (Fidelity, Diversity)\n",
                "    dist_res, struct_res, fin_res = wrappers.run_advanced_metrics(real_data, generated_data)\n",
                "    mem_ratio, div_res, fld_score, dcr, prec, recall, mmd = wrappers.run_new_metrics(real_data, generated_data)\n",
                "    \n",
                "    # Store results\n",
                "    metrics_dict['dist_results'] = dist_res\n",
                "    metrics_dict['struct_results'] = struct_res\n",
                "    metrics_dict['fin_results'] = fin_res\n",
                "    metrics_dict['mem_ratio'] = mem_ratio\n",
                "    metrics_dict['div_results'] = div_res\n",
                "    metrics_dict['fld_score'] = fld_score\n",
                "    metrics_dict['dcr_score'] = dcr\n",
                "    metrics_dict['precision'] = prec\n",
                "    metrics_dict['recall'] = recall\n",
                "    metrics_dict['mmd_score'] = mmd\n",
                "    \n",
                "    # Display Scorecard (Dollar)\n",
                "    print(\"\\n--- DOLLAR SPACE SCORECARD ---\")\n",
                "    dollar_scorecard = report.generate_summary_scorecard(metrics_dict)\n",
                "    display(dollar_scorecard)\n",
                "\n",
                "# ==========================================\n",
                "# 2. REPARAMETERIZED SPACE EVALUATION\n",
                "# ==========================================\n",
                "if HAS_NORM_DATA:\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"EVALUATION: REPARAMETERIZED SPACE (Norm Features)\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    norm_metrics_dict = {}\n",
                "    \n",
                "    # 1. OHLC Invariants (Wick Non-negativity)\n",
                "    # Assuming index 2=HighNorm, 3=LowNorm based on standard reparam\n",
                "    if generated_data_norm.shape[2] >= 4:\n",
                "        wick_high = generated_data_norm[:, :, 2]\n",
                "        wick_low = generated_data_norm[:, :, 3]\n",
                "        ohlc_valid_pct = ((wick_high >= -1e-5) & (wick_low >= -1e-5)).mean() * 100\n",
                "        print(f\"OHLC Invariants: {ohlc_valid_pct:.1f}% valid (Wicks >= 0)\")\n",
                "    \n",
                "    # 2. Run Full Advanced Suite on Norm Data\n",
                "    # Use raw norm data for distribution checks\n",
                "    dist_res_n, struct_res_n, fin_res_n = wrappers.run_advanced_metrics(real_data_norm, generated_data_norm)\n",
                "    mem_ratio_n, div_res_n, fld_score_n, dcr_n, prec_n, recall_n, mmd_n = wrappers.run_new_metrics(real_data_norm, generated_data_norm)\n",
                "    \n",
                "    # Store in separate dict\n",
                "    norm_metrics_dict['dist_results'] = dist_res_n\n",
                "    norm_metrics_dict['struct_results'] = struct_res_n\n",
                "    norm_metrics_dict['fin_results'] = fin_res_n\n",
                "    norm_metrics_dict['mem_ratio'] = mem_ratio_n\n",
                "    norm_metrics_dict['div_results'] = div_res_n\n",
                "    norm_metrics_dict['fld_score'] = fld_score_n\n",
                "    norm_metrics_dict['dcr_score'] = dcr_n\n",
                "    norm_metrics_dict['precision'] = prec_n\n",
                "    norm_metrics_dict['recall'] = recall_n\n",
                "    norm_metrics_dict['mmd_score'] = mmd_n\n",
                "    \n",
                "    # Discriminative on Reparam (Texture check)\n",
                "    # Use scaled norm data if needed, but norm data is usually already small scale. \n",
                "    # Let's use real_data_norm directly as it's the raw output.\n",
                "    disc_score_n = wrappers.run_discriminative_benchmark(real_data_norm, generated_data_norm, iterations=1)\n",
                "    norm_metrics_dict['discriminative_score'] = disc_score_n\n",
                "    \n",
                "    # Display Scorecard (Reparam)\n",
                "    print(\"\\n--- REPARAMETERIZED SPACE SCORECARD ---\")\n",
                "    norm_scorecard = report.generate_summary_scorecard(norm_metrics_dict)\n",
                "    display(norm_scorecard)\n",
                "else:\n",
                "    print(\"⚠️ Reparameterized data not available. Skipping norm space metrics.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature_stats_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Detailed Feature Stats (Dollar Space)\n",
                "report.display_feature_stats(real_stat_det, gen_stat_det)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# WaveletDiff Evaluation\n",
                "\n",
                "This notebook evaluates a trained WaveletDiff model directly from your Google Colab environment using a compressed experiment archive from Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import os\n",
                "\n",
                "# --- Drive Paths ---\n",
                "DRIVE_MOUNT_PATH = \"/content/drive\" # @param {type:\"string\"}\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/waveletDiff_experiments\" # @param {type:\"string\"}\n",
                "CHECKPOINT_FOLDER = \"checkpoints\" # @param {type:\"string\"}\n",
                "SAMPLES_FOLDER = \"samples\" # @param {type:\"string\"}\n",
                "\n",
                "MODEL_FILENAME = \"stocks_experiment.tar.gz\" # @param {type:\"string\"}\n",
                "MODEL_BASENAME = MODEL_FILENAME.replace('.tar.gz', '').replace('.zip', '').replace('.ckpt', '').replace('.tgz', '').replace('.gz', '')\n",
                "\n",
                "DRIVE_CHECKPOINT_PATH = os.path.join(DRIVE_BASE_PATH, CHECKPOINT_FOLDER, MODEL_FILENAME)\n",
                "DRIVE_SAMPLES_PATH = os.path.join(DRIVE_BASE_PATH, SAMPLES_FOLDER, MODEL_BASENAME)\n",
                "\n",
                "# --- Repository Settings ---\n",
                "REPO_BRANCH = \"develop\" # @param {type:\"string\"}\n",
                "\n",
                "# --- Evaluation Settings ---\n",
                "DATASET = \"stocks\" # @param {type:\"string\"}\n",
                "EXPERIMENT_NAME = \"evaluation_run\" # @param {type:\"string\"}\n",
                "NUM_SAMPLES = 2000 # @param {type:\"integer\"}\n",
                "SAMPLING_METHOD = \"ddpm\" # @param [\"ddpm\", \"ddim\"]\n",
                "COMPILE_MODE = \"none\" # @param [\"none\", \"default\", \"reduce-overhead\", \"max-autotune\"]\n",
                "DEVICE = \"cuda\" # @param [\"cuda\", \"cpu\"]\n",
                "\n",
                "# --- Evaluation Options ---\n",
                "EXCLUDE_VOLUME = True # @param {type:\"boolean\"}\n",
                "CACHE_SAMPLES_TO_DRIVE = True # @param {type:\"boolean\"}\n",
                "USE_CACHED_SAMPLES = True # @param {type:\"boolean\"}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Setup (Clone, Install, Mount)\n",
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "import zipfile\n",
                "import tarfile\n",
                "from google.colab import drive\n",
                "\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_NAME = \"waveletDiff_synth_data\"\n",
                "REPO_PATH = os.path.abspath(REPO_NAME)\n",
                "\n",
                "# 1. Clone or Pull Repo\n",
                "if os.path.exists(REPO_PATH):\n",
                "    nested_path = os.path.join(REPO_PATH, REPO_NAME)\n",
                "    if os.path.exists(nested_path):\n",
                "        print(f\"⚠️ Detected nested repository at {nested_path}. cleaning up...\")\n",
                "        shutil.rmtree(REPO_PATH)\n",
                "        print(f\"Cloning {REPO_URL}...\")\n",
                "        !git clone {REPO_URL} {REPO_NAME}\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "    else:\n",
                "        print(f\"Pulling latest for {REPO_NAME}...\")\n",
                "        !git -C {REPO_NAME} fetch origin\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "        !git -C {REPO_NAME} pull origin {REPO_BRANCH}\n",
                "else:\n",
                "    print(f\"Cloning {REPO_URL}...\")\n",
                "    !git clone {REPO_URL} {REPO_NAME}\n",
                "    !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "\n",
                "# 2. Install Dependencies\n",
                "!pip install -q pytorch-lightning pywavelets scipy pandas tqdm scikit-learn tslearn seaborn statsmodels\n",
                "\n",
                "# 3. Mount Google Drive\n",
                "if not os.path.exists(DRIVE_MOUNT_PATH):\n",
                "    drive.mount(DRIVE_MOUNT_PATH)\n",
                "\n",
                "# 4. Setup Paths\n",
                "if os.path.join(REPO_PATH, \"src\") not in sys.path:\n",
                "    sys.path.append(os.path.join(REPO_PATH, \"src\"))\n",
                "if os.path.join(REPO_PATH, \"src\", \"evaluation\") not in sys.path:\n",
                "    sys.path.append(os.path.join(REPO_PATH, \"src\", \"evaluation\"))\n",
                "\n",
                "# 5. Prepare Checkpoint (Handle .zip, .tar.gz, .tgz, .gz, .ckpt)\n",
                "local_exp_dir = os.path.join(REPO_PATH, \"outputs\", EXPERIMENT_NAME)\n",
                "os.makedirs(local_exp_dir, exist_ok=True)\n",
                "\n",
                "if os.path.exists(DRIVE_CHECKPOINT_PATH):\n",
                "    print(f\"Found model file at {DRIVE_CHECKPOINT_PATH}\")\n",
                "    \n",
                "    if DRIVE_CHECKPOINT_PATH.endswith((\".zip\", \".tar.gz\", \".tgz\", \".gz\")):\n",
                "        print(f\"Unpacking archive to {local_exp_dir}...\")\n",
                "        try:\n",
                "            if DRIVE_CHECKPOINT_PATH.endswith(\".gz\") and not DRIVE_CHECKPOINT_PATH.endswith(\".tar.gz\"):\n",
                "                # Training notebook uses shutil.make_archive with 'gztar', so it is a tarball even if named .gz\n",
                "                shutil.unpack_archive(DRIVE_CHECKPOINT_PATH, local_exp_dir, format='gztar')\n",
                "            else:\n",
                "                shutil.unpack_archive(DRIVE_CHECKPOINT_PATH, local_exp_dir)\n",
                "            print(f\"✅ Successfully unpacked archive!\")\n",
                "        except Exception as e:\n",
                "            print(f\"❌ Error unpacking archive: {e}\")\n",
                "            \n",
                "    elif DRIVE_CHECKPOINT_PATH.endswith(\".ckpt\"):\n",
                "        print(\"Detected direct .ckpt file. Copying...\")\n",
                "        dst_ckpt = os.path.join(local_exp_dir, \"checkpoint.ckpt\")\n",
                "        try:\n",
                "            shutil.copy2(DRIVE_CHECKPOINT_PATH, dst_ckpt)\n",
                "            print(f\"✅ Copied checkpoint to {dst_ckpt}\")\n",
                "        except Exception as e:\n",
                "            print(f\"❌ Error copying checkpoint: {e}\")\n",
                "    else:\n",
                "        print(f\"⚠️ Unknown file extension. Trying unpack anyway...\")\n",
                "        try:\n",
                "             shutil.unpack_archive(DRIVE_CHECKPOINT_PATH, local_exp_dir)\n",
                "             print(f\"✅ Successfully unpacked archive!\")\n",
                "        except:\n",
                "             print(f\"❌ Could not unpack or identify file format.\")\n",
                "\n",
                "    if os.path.exists(os.path.join(local_exp_dir, \"checkpoint.ckpt\")):\n",
                "         print(\"✅ Validated checkpoint.ckpt exists\")\n",
                "    else:\n",
                "         print(\"⚠️ WARNING: checkpoint.ckpt not found.\")\n",
                "else:\n",
                "    print(f\"❌ ERROR: Model file not found at {DRIVE_CHECKPOINT_PATH}\")\n",
                "\n",
                "# Sync configs\n",
                "configs_src = os.path.join(REPO_PATH, \"WaveletDiff_source\", \"configs\")\n",
                "configs_dst = os.path.join(REPO_PATH, \"configs\")\n",
                "\n",
                "if os.path.exists(configs_src):\n",
                "    if os.path.exists(configs_dst):\n",
                "        shutil.rmtree(configs_dst)\n",
                "    shutil.copytree(configs_src, configs_dst)\n",
                "    print(\"✅ Configs synced to repo root\")\n",
                "\n",
                "print(\"✅ Setup Complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gen_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Generate or Load Samples\n",
                "import os\n",
                "import numpy as np\n",
                "os.chdir(os.path.join(REPO_PATH, \"src\"))\n",
                "\n",
                "# Define paths\n",
                "local_gen_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples.npy\"\n",
                "local_real_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples.npy\"\n",
                "\n",
                "drive_gen_path = os.path.join(DRIVE_SAMPLES_PATH, f\"{SAMPLING_METHOD}_samples.npy\")\n",
                "drive_real_path = os.path.join(DRIVE_SAMPLES_PATH, \"real_samples.npy\")\n",
                "\n",
                "samples_loaded = False\n",
                "\n",
                "# Try to load cached samples from Drive\n",
                "if USE_CACHED_SAMPLES and os.path.exists(drive_gen_path) and os.path.exists(drive_real_path):\n",
                "    print(f\"✅ Found cached samples in Drive: {DRIVE_SAMPLES_PATH}\")\n",
                "    print(\"Loading cached samples...\")\n",
                "    \n",
                "    os.makedirs(os.path.dirname(local_gen_path), exist_ok=True)\n",
                "    shutil.copy2(drive_gen_path, local_gen_path)\n",
                "    shutil.copy2(drive_real_path, local_real_path)\n",
                "    \n",
                "    _gen = np.load(local_gen_path)\n",
                "    _real = np.load(local_real_path)\n",
                "    print(f\"Loaded Generated: {_gen.shape}, Real: {_real.shape}\")\n",
                "    samples_loaded = True\n",
                "\n",
                "# Generate samples if not cached\n",
                "if not samples_loaded:\n",
                "    print(f\"Generating {NUM_SAMPLES} samples using {SAMPLING_METHOD} with compile_mode={COMPILE_MODE}...\")\n",
                "\n",
                "    status = !python sample.py \\\n",
                "        --experiment_name {EXPERIMENT_NAME} \\\n",
                "        --dataset {DATASET} \\\n",
                "        --num_samples {NUM_SAMPLES} \\\n",
                "        --sampling_method {SAMPLING_METHOD} \\\n",
                "        --compile_mode {COMPILE_MODE}\n",
                "\n",
                "    print(\"\\n\".join(status))\n",
                "\n",
                "    if any(\"Traceback\" in s or \"Error\" in s for s in status):\n",
                "        print(\"\\n❌ Generation failed. Check the error log above.\")\n",
                "    else:\n",
                "        print(\"\\n✅ Sampling Complete\")\n",
                "        \n",
                "        # Save to Drive if configured\n",
                "        if CACHE_SAMPLES_TO_DRIVE:\n",
                "            print(f\"Saving samples to Drive: {DRIVE_SAMPLES_PATH}...\")\n",
                "            os.makedirs(DRIVE_SAMPLES_PATH, exist_ok=True)\n",
                "            shutil.copy2(local_gen_path, drive_gen_path)\n",
                "            shutil.copy2(local_real_path, drive_real_path)\n",
                "            print(\"✅ Samples saved to Drive!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d600f08e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Imports and Setup\n",
                "import sys\n",
                "import numpy as np\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.decomposition import PCA\n",
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "from discriminative_metrics import discriminative_score_metrics\n",
                "from predictive_metrics import predictive_score_metrics\n",
                "from context_fid import Context_FID\n",
                "from cross_correlation import CrossCorrelLoss\n",
                "from metric_utils import display_scores\n",
                "from dtw import dtw_js_divergence_distance\n",
                "from advanced_metrics import calculate_distribution_fidelity, calculate_structural_alignment, calculate_financial_reality, calculate_memorization_ratio, calculate_diversity_metrics, calculate_fld\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_context(\"notebook\", font_scale=1.2)\n",
                "COLORS = {\"Real\": \"#d62728\", \"Generated\": \"#1f77b4\"}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b5f4a164",
            "metadata": {},
            "source": [
                "### Load Real and Generated Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "96588f6e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths are relative to 'src' directory where we currently are\n",
                "real_data_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples.npy\"\n",
                "gen_data_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples.npy\"\n",
                "\n",
                "if not os.path.exists(real_data_path):\n",
                "    raise FileNotFoundError(f\"Could not find real samples at {os.path.abspath(real_data_path)}. Check Cell 3 output.\")\n",
                "\n",
                "real_data = np.load(real_data_path)\n",
                "generated_data = np.load(gen_data_path)\n",
                "print(f\"Loaded Real: {real_data.shape}, Generated: {generated_data.shape}\")\n",
                "\n",
                "# Optionally exclude volume (last feature)\n",
                "if EXCLUDE_VOLUME and real_data.shape[2] > 1:\n",
                "    print(f\"⚠️ Excluding volume feature (last dimension). D: {real_data.shape[2]} -> {real_data.shape[2]-1}\")\n",
                "    real_data = real_data[:, :, :-1]\n",
                "    generated_data = generated_data[:, :, :-1]\n",
                "    print(f\"New shapes: Real: {real_data.shape}, Generated: {generated_data.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6fb459a",
            "metadata": {},
            "outputs": [],
            "source": [
                "num_samples = min(real_data.shape[0], generated_data.shape[0])\n",
                "if real_data.shape[0] > num_samples:\n",
                "    print(f\"WARNING: Using all {num_samples} generated samples for evaluation.\")\n",
                "else:\n",
                "    print(f\"Number of samples: {num_samples}\")\n",
                "\n",
                "random_indices = np.random.choice(len(real_data), num_samples, replace=False)\n",
                "real_data = real_data[random_indices]\n",
                "random_indices = np.random.choice(len(generated_data), num_samples, replace=False)\n",
                "generated_data = generated_data[random_indices]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71296ef8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# minmax scale the inputs for fair comparison\n",
                "data_min = np.min(real_data, axis=(0,1), keepdims=True)\n",
                "data_max = np.max(real_data, axis=(0,1), keepdims=True)\n",
                "\n",
                "real_data = (real_data - data_min) / (data_max - data_min + 1e-8)\n",
                "generated_data = (generated_data - data_min) / (data_max - data_min + 1e-8)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vis_md",
            "metadata": {},
            "source": [
                "### Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_tsne",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title t-SNE & PCA Visualization\n",
                "\n",
                "def plot_distribution_reduction(real, generated, n_samples=1000):\n",
                "    n_samples = min(n_samples, len(real), len(generated))\n",
                "    \n",
                "    real_flat = real[:n_samples].reshape(n_samples, -1)\n",
                "    gen_flat = generated[:n_samples].reshape(n_samples, -1)\n",
                "    \n",
                "    data = np.concatenate([real_flat, gen_flat], axis=0)\n",
                "    \n",
                "    print(\"Running t-SNE...\")\n",
                "    tsne = TSNE(n_components=2, perplexity=40, n_iter=300)\n",
                "    tsne_results = tsne.fit_transform(data)\n",
                "    \n",
                "    plt.figure(figsize=(16, 6))\n",
                "    \n",
                "    plt.subplot(1, 2, 1)\n",
                "    sns.scatterplot(x=tsne_results[:n_samples, 0], y=tsne_results[:n_samples, 1], \n",
                "                    color=COLORS[\"Real\"], alpha=0.3, label=\"Real\", s=20)\n",
                "    sns.scatterplot(x=tsne_results[n_samples:, 0], y=tsne_results[n_samples:, 1],\n",
                "                    color=COLORS[\"Generated\"], alpha=0.3, label=\"Generated\", s=20)\n",
                "    plt.title(\"t-SNE Visualization\")\n",
                "    plt.legend()\n",
                "    \n",
                "    print(\"Running PCA...\")\n",
                "    pca = PCA(n_components=2)\n",
                "    pca_results = pca.fit_transform(data)\n",
                "    \n",
                "    plt.subplot(1, 2, 2)\n",
                "    sns.scatterplot(x=pca_results[:n_samples, 0], y=pca_results[:n_samples, 1], \n",
                "                    color=COLORS[\"Real\"], alpha=0.3, label=\"Real\", s=20)\n",
                "    sns.scatterplot(x=pca_results[n_samples:, 0], y=pca_results[n_samples:, 1],\n",
                "                    color=COLORS[\"Generated\"], alpha=0.3, label=\"Generated\", s=20)\n",
                "    plt.title(\"PCA Visualization\")\n",
                "    plt.legend()\n",
                "    \n",
                "    plt.show()\n",
                "\n",
                "plot_distribution_reduction(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_pdf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Probability Density Function (Data Values)\n",
                "\n",
                "def plot_pdf(real, generated):\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    sns.kdeplot(real.flatten(), fill=True, color=COLORS[\"Real\"], label=\"Real\", alpha=0.3)\n",
                "    sns.kdeplot(generated.flatten(), fill=True, color=COLORS[\"Generated\"], label=\"Generated\", alpha=0.3)\n",
                "    \n",
                "    plt.title(\"Probability Density Function (All Values)\")\n",
                "    plt.xlabel(\"Data Value\")\n",
                "    plt.ylabel(\"Density\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "plot_pdf(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_samples",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Sample Visualization (Generated vs Real)\n",
                "\n",
                "def plot_samples(real, generated, n_samples=5):\n",
                "    n_features = real.shape[2]\n",
                "    \n",
                "    fig, axes = plt.subplots(2, n_samples, figsize=(n_samples * 4, 6), sharey=True)\n",
                "    \n",
                "    for i in range(n_samples):\n",
                "        for f in range(n_features):\n",
                "            axes[0, i].plot(real[i, :, f], alpha=0.8)\n",
                "        axes[0, i].set_title(f\"Real Sample {i}\")\n",
                "        if i == 0: axes[0, i].set_ylabel(\"Value (MinMax Scaled)\")\n",
                "        \n",
                "        for f in range(n_features):\n",
                "            axes[1, i].plot(generated[i, :, f], alpha=0.8)\n",
                "        axes[1, i].set_title(f\"Gen Sample {i}\")\n",
                "        if i == 0: axes[1, i].set_ylabel(\"Value (MinMax Scaled)\")\n",
                "    \n",
                "    from matplotlib.lines import Line2D\n",
                "    lines = [Line2D([0], [0], color=f\"C{i}\", lw=2) for i in range(n_features)]\n",
                "    fig.legend(lines, [f\"Feature {i}\" for i in range(n_features)], loc='lower center', ncol=n_features)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "plot_samples(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics_md",
            "metadata": {},
            "source": [
                "### Statistical Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "stat_metrics",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Statistical Distribution Metrics (Real vs Generated Comparison)\n",
                "from scipy.stats import skew, kurtosis\n",
                "\n",
                "def calculate_statistical_metrics(real, generated):\n",
                "    r_flat = real.reshape(-1, real.shape[2])\n",
                "    g_flat = generated.reshape(-1, generated.shape[2])\n",
                "    n_features = real.shape[2]\n",
                "    \n",
                "    real_stats = {\n",
                "        \"Mean\": np.mean(r_flat, axis=0),\n",
                "        \"Std\": np.std(r_flat, axis=0),\n",
                "        \"Skewness\": skew(r_flat, axis=0),\n",
                "        \"Kurtosis\": kurtosis(r_flat, axis=0),\n",
                "        \"Min\": np.min(r_flat, axis=0),\n",
                "        \"Max\": np.max(r_flat, axis=0),\n",
                "    }\n",
                "    \n",
                "    gen_stats = {\n",
                "        \"Mean\": np.mean(g_flat, axis=0),\n",
                "        \"Std\": np.std(g_flat, axis=0),\n",
                "        \"Skewness\": skew(g_flat, axis=0),\n",
                "        \"Kurtosis\": kurtosis(g_flat, axis=0),\n",
                "        \"Min\": np.min(g_flat, axis=0),\n",
                "        \"Max\": np.max(g_flat, axis=0),\n",
                "    }\n",
                "    \n",
                "    print(\"=\" * 80)\n",
                "    print(\"STATISTICAL COMPARISON: Real vs Generated Data\")\n",
                "    print(\"=\" * 80)\n",
                "    \n",
                "    for f in range(n_features):\n",
                "        print(f\"\\n--- Feature {f} ---\")\n",
                "        print(f\"{'Metric':<15} {'Real':>12} {'Generated':>12} {'Diff (Abs)':>12}\")\n",
                "        print(\"-\" * 55)\n",
                "        for stat_name in real_stats:\n",
                "            r_val = real_stats[stat_name][f]\n",
                "            g_val = gen_stats[stat_name][f]\n",
                "            diff = abs(r_val - g_val)\n",
                "            print(f\"{stat_name:<15} {r_val:>12.4f} {g_val:>12.4f} {diff:>12.4f}\")\n",
                "    \n",
                "    aggregate = {\n",
                "        \"Mean MAE\": np.mean(np.abs(real_stats[\"Mean\"] - gen_stats[\"Mean\"])),\n",
                "        \"Std MAE\": np.mean(np.abs(real_stats[\"Std\"] - gen_stats[\"Std\"])),\n",
                "        \"Skewness MAE\": np.mean(np.abs(real_stats[\"Skewness\"] - gen_stats[\"Skewness\"])),\n",
                "        \"Kurtosis MAE\": np.mean(np.abs(real_stats[\"Kurtosis\"] - gen_stats[\"Kurtosis\"])),\n",
                "    }\n",
                "    \n",
                "    return aggregate, real_stats, gen_stats\n",
                "\n",
                "stat_results, real_stats_detail, gen_stats_detail = calculate_statistical_metrics(real_data, generated_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "da09d10a",
            "metadata": {},
            "source": [
                "### Discriminative Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5b47b22",
            "metadata": {},
            "outputs": [],
            "source": [
                "iterations = 5\n",
                "discriminative_score = []\n",
                "\n",
                "for i in range(iterations):\n",
                "    temp_disc, fake_acc, real_acc = discriminative_score_metrics(real_data, generated_data)\n",
                "    discriminative_score.append(temp_disc)\n",
                "    print(f'Iter {i}: ', temp_disc, '\\n')\n",
                "      \n",
                "display_scores(discriminative_score)\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32ec7287",
            "metadata": {},
            "source": [
                "### Predictive Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d766f30c",
            "metadata": {},
            "outputs": [],
            "source": [
                "iterations = 5\n",
                "predictive_score = []\n",
                "for i in range(iterations):\n",
                "    temp_pred = predictive_score_metrics(real_data, generated_data)\n",
                "    predictive_score.append(temp_pred)\n",
                "    print(i, ' epoch: ', temp_pred, '\\n')\n",
                "      \n",
                "display_scores(predictive_score)\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18a2b417",
            "metadata": {},
            "source": [
                "### Context-FID Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2bdc9d5e",
            "metadata": {},
            "outputs": [],
            "source": [
                "context_fid_score = []\n",
                "\n",
                "for i in range(iterations):\n",
                "    context_fid = Context_FID(real_data, generated_data)\n",
                "    context_fid_score.append(context_fid)\n",
                "    print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
                "      \n",
                "display_scores(context_fid_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8ea6fe97",
            "metadata": {},
            "source": [
                "### Correlational Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64dba75d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def random_choice(size, num_select=100):\n",
                "    select_idx = np.random.randint(low=0, high=size, size=(num_select,))\n",
                "    return select_idx\n",
                "\n",
                "x_real = torch.from_numpy(real_data)\n",
                "x_fake = torch.from_numpy(generated_data)\n",
                "\n",
                "correlational_score = []\n",
                "size = 1000\n",
                "\n",
                "for i in range(iterations):\n",
                "    real_idx = random_choice(x_real.shape[0], size)\n",
                "    fake_idx = random_choice(x_fake.shape[0], size)\n",
                "    corr = CrossCorrelLoss(x_real[real_idx, :, :], name='CrossCorrelLoss')\n",
                "    loss = corr.compute(x_fake[fake_idx, :, :])\n",
                "    correlational_score.append(loss.item())\n",
                "    print(f'Iter {i}: ', 'cross-correlation =', loss.item(), '\\n')\n",
                "\n",
                "display_scores(correlational_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4237eebe",
            "metadata": {},
            "source": [
                "### DTW distance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cc548e60",
            "metadata": {},
            "outputs": [],
            "source": [
                "iterations = 5\n",
                "js_results = []\n",
                "for i in range(iterations):\n",
                "    js_dist = dtw_js_divergence_distance(real_data, generated_data, n_samples=100)['js_divergence']\n",
                "    print(\"js_dist: \", round(js_dist, 4))\n",
                "    js_results.append(js_dist)\n",
                "display_scores(js_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "adv_metrics_md",
            "metadata": {},
            "source": [
                "### Advanced Financial Metrics (Wasserstein, KS, PCA, ACF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "adv_metrics_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Run Advanced Metrics\n",
                "print(\"Running Distribution Fidelity Checks (Wasserstein, KS Test)...\")\n",
                "dist_results = calculate_distribution_fidelity(real_data, generated_data)\n",
                "print(\"Distribution Fidelity:\", dist_results)\n",
                "\n",
                "print(\"\\nRunning Structural Alignment Checks (PCA, t-SNE)...\")\n",
                "struct_results = calculate_structural_alignment(real_data, generated_data)\n",
                "print(\"Structural Alignment:\", struct_results)\n",
                "\n",
                "print(\"\\nRunning Financial Reality Checks (ACF, Cross-Corr, Volatility)...\")\n",
                "fin_results = calculate_financial_reality(real_data, generated_data)\n",
                "print(\"Financial Reality:\", fin_results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "new_metrics_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Run New Advanced Metrics (Memorization, Diversity, FLD)\n",
                "print(\"\\nRunning Memorization Check (1/3 Rule)...\")\n",
                "mem_ratio = calculate_memorization_ratio(real_data, generated_data)\n",
                "print(f\"Memorization Ratio: {mem_ratio:.4f}\")\n",
                "\n",
                "print(\"\\nRunning Diversity Check (Coverage)...\")\n",
                "div_results = calculate_diversity_metrics(real_data, generated_data)\n",
                "print(f\"Diversity Metrics: {div_results}\")\n",
                "\n",
                "print(\"\\nRunning Feature Likelihood Divergence (FLD)...\")\n",
                "fld_score = calculate_fld(real_data, generated_data)\n",
                "print(f\"FLD Score: {fld_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary_md",
            "metadata": {},
            "source": [
                "### Centralized Metric Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "summary_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Summary Scorecard\n",
                "import pandas as pd\n",
                "\n",
                "summary_data = []\n",
                "\n",
                "# 1. Statistical (Aggregated MAEs from stat_results)\n",
                "for k, v in stat_results.items():\n",
                "    summary_data.append({\"Category\": \"Statistical (MAE)\", \"Metric\": k, \"Value\": v, \"Goal\": \"lower\", \"Description\": \"Diff in statistical moments (Mean/Std/Skew/Kurt).\"})\n",
                "\n",
                "# 2. Discriminative & Predictive\n",
                "summary_data.append({\"Category\": \"Model Quality\", \"Metric\": \"Discriminative Score\", \"Value\": np.mean(discriminative_score), \"Goal\": \"lower\", \"Description\": \"Classifier accuracy deviation from 0.5 (Real vs Fake).\"})\n",
                "summary_data.append({\"Category\": \"Model Quality\", \"Metric\": \"Predictive Score\", \"Value\": np.mean(predictive_score), \"Goal\": \"lower\", \"Description\": \"MAE of TSTR (Train on Synthetic, Test on Real).\"})\n",
                "summary_data.append({\"Category\": \"Model Quality\", \"Metric\": \"Context-FID\", \"Value\": np.mean(context_fid_score), \"Goal\": \"lower\", \"Description\": \"FID score on embeddings (e.g. Inception/Transformer).\"})\n",
                "summary_data.append({\"Category\": \"Model Quality\", \"Metric\": \"Cross-Correl Loss\", \"Value\": np.mean(correlational_score), \"Goal\": \"lower\", \"Description\": \"Difference in cross-correlation matrices.\"})\n",
                "summary_data.append({\"Category\": \"Model Quality\", \"Metric\": \"DTW (JS Divergence)\", \"Value\": np.mean(js_results), \"Goal\": \"lower\", \"Description\": \"DTW-based distribution distance.\"})\n",
                "\n",
                "# 3. Distribution Fidelity\n",
                "summary_data.append({\"Category\": \"Distribution Fidelity\", \"Metric\": \"Wasserstein (Mean)\", \"Value\": dist_results[\"Wasserstein_Mean\"], \"Goal\": \"lower\", \"Description\": \"Earth Mover's Distance between features.\"})\n",
                "summary_data.append({\"Category\": \"Distribution Fidelity\", \"Metric\": \"KS Test Stat (Mean)\", \"Value\": dist_results[\"KS_Stat_Mean\"], \"Goal\": \"lower\", \"Description\": \"Kolmogorov-Smirnov statistic (max diff in CDF).\"})\n",
                "summary_data.append({\"Category\": \"Distribution Fidelity\", \"Metric\": \"KS P-Value (Mean)\", \"Value\": dist_results[\"KS_PVal_Mean\"], \"Goal\": \"higher\", \"Description\": \"Statistical significance of KS test.\"})\n",
                "\n",
                "# 4. Structural Alignment\n",
                "summary_data.append({\"Category\": \"Structural Alignment\", \"Metric\": \"PCA EVR Correlation\", \"Value\": struct_results[\"PCA_EVR_Corr\"], \"Goal\": \"higher\", \"Description\": \"Correlation of PCA Explained Variance Ratios.\"})\n",
                "# t-SNE 1-NN: ideal is 0.5, so we show distance from 0.5\n",
                "tsne_val = struct_results[\"tSNE_1NN_Acc\"]\n",
                "tsne_error = abs(tsne_val - 0.5)\n",
                "summary_data.append({\"Category\": \"Structural Alignment\", \"Metric\": \"t-SNE 1-NN (|x-0.5|)\", \"Value\": tsne_error, \"Goal\": \"lower\", \"Description\": \"Classifier accuracy in t-SNE space (Ideal=0.5).\"})\n",
                "\n",
                "# 5. Financial Reality\n",
                "summary_data.append({\"Category\": \"Financial Reality\", \"Metric\": \"ACF MSE (Lags 1,5,20)\", \"Value\": fin_results[\"ACF_MSE\"], \"Goal\": \"lower\", \"Description\": \"MSE of Autocorrelation Functions.\"})\n",
                "summary_data.append({\"Category\": \"Financial Reality\", \"Metric\": \"Cross-Corr Matrix Diff\", \"Value\": fin_results[\"CrossCorr_Norm_Diff\"], \"Goal\": \"lower\", \"Description\": \"Norm difference of correlation matrices.\"})\n",
                "summary_data.append({\"Category\": \"Financial Reality\", \"Metric\": \"Volatility Clustering MSE\", \"Value\": fin_results[\"Volatility_MSE\"], \"Goal\": \"lower\", \"Description\": \"MSE of squared returns ACF (Volatility).\"})\n",
                "\n",
                "# 6. New Metrics\n",
                "summary_data.append({\"Category\": \"New Metrics\", \"Metric\": \"Memorization Ratio (1/3 Rule)\", \"Value\": mem_ratio, \"Goal\": \"lower\", \"Description\": \"Fraction of samples that are near-duplicates of training data.\"})\n",
                "summary_data.append({\"Category\": \"New Metrics\", \"Metric\": \"Diversity (Coverage)\", \"Value\": div_results[\"Coverage\"], \"Goal\": \"higher\", \"Description\": \"Fraction of real data covered by synthetic samples.\"})\n",
                "summary_data.append({\"Category\": \"New Metrics\", \"Metric\": \"FLD (Likelihood Divergence)\", \"Value\": fld_score, \"Goal\": \"lower\", \"Description\": \"Divergence in likelihood under real data density (GMM).\"})\n",
                "\n",
                "# Create DataFrame\n",
                "df_results = pd.DataFrame(summary_data)\n",
                "\n",
                "# Custom styling based on Goal (with black text for readability)\n",
                "def style_value(row):\n",
                "    val = row['Value']\n",
                "    goal = row['Goal']\n",
                "    \n",
                "    if goal == 'higher':\n",
                "        # Higher is better: Green for high, Red for low\n",
                "        if val >= 0.99: return 'background-color: #2ecc71; color: black'\n",
                "        elif val >= 0.9: return 'background-color: #82e0aa; color: black'\n",
                "        elif val >= 0.7: return 'background-color: #f9e79f; color: black'\n",
                "        else: return 'background-color: #e74c3c; color: black'\n",
                "    else:\n",
                "        # Lower is better: Green for low, Red for high  \n",
                "        if val <= 0.01: return 'background-color: #2ecc71; color: black'\n",
                "        elif val <= 0.05: return 'background-color: #82e0aa; color: black'\n",
                "        elif val <= 0.15: return 'background-color: #f9e79f; color: black'\n",
                "        elif val <= 0.3: return 'background-color: #f5b041; color: black'\n",
                "        else: return 'background-color: #e74c3c; color: black'\n",
                "\n",
                "styled = df_results.style.apply(lambda row: [style_value(row) if col == 'Value' else '' for col in df_results.columns], axis=1)\n",
                "pd.set_option('display.max_rows', None)\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "display(styled)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "feature_detail_md",
            "metadata": {},
            "source": [
                "### Per-Feature Statistical Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature_detail_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Feature-Level Stats Table\n",
                "n_features = len(real_stats_detail['Mean'])\n",
                "feature_data = []\n",
                "\n",
                "for f in range(n_features):\n",
                "    for stat_name in real_stats_detail:\n",
                "        r_val = real_stats_detail[stat_name][f]\n",
                "        g_val = gen_stats_detail[stat_name][f]\n",
                "        diff = abs(r_val - g_val)\n",
                "        feature_data.append({\n",
                "            \"Feature\": f,\n",
                "            \"Stat\": stat_name,\n",
                "            \"Real\": r_val,\n",
                "            \"Synthetic\": g_val,\n",
                "            \"Abs Diff\": diff\n",
                "        })\n",
                "\n",
                "df_features = pd.DataFrame(feature_data)\n",
                "display(df_features.style.background_gradient(cmap='RdYlGn_r', subset=['Abs Diff']))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
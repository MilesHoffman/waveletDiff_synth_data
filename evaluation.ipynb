{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# WaveletDiff Evaluation (Refactored)\n",
                "\n",
                "This notebook evaluates a trained WaveletDiff model. It acts as a frontend interface, delegating heavy logic to `src/evaluation` modules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import os\n",
                "\n",
                "# --- Drive Paths ---\n",
                "DRIVE_MOUNT_PATH = \"/content/drive\" # @param {type:\"string\"}\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/personal_drive/trading/waveletDiff\" # @param {type:\"string\"}\n",
                "CHECKPOINT_FOLDER = \"checkpoints\" # @param {type:\"string\"}\n",
                "SAMPLES_FOLDER = \"samples\" # @param {type:\"string\"}\n",
                "\n",
                "MODEL_FILENAME = \"stocks_experiment.tar.gz\" # @param {type:\"string\"}\n",
                "MODEL_BASENAME = MODEL_FILENAME.replace('.tar.gz', '').replace('.zip', '').replace('.ckpt', '').replace('.tgz', '').replace('.gz', '')\n",
                "\n",
                "DRIVE_CHECKPOINT_PATH = os.path.join(DRIVE_BASE_PATH, CHECKPOINT_FOLDER, MODEL_FILENAME)\n",
                "DRIVE_SAMPLES_PATH = os.path.join(DRIVE_BASE_PATH, SAMPLES_FOLDER, MODEL_BASENAME)\n",
                "\n",
                "# --- Repository Settings ---\n",
                "REPO_BRANCH = \"develop\" # @param {type:\"string\"}\n",
                "\n",
                "# --- Evaluation Settings ---\n",
                "DATASET = \"stocks\" # @param {type:\"string\"}\n",
                "EXPERIMENT_NAME = \"evaluation_run\" # @param {type:\"string\"}\n",
                "NUM_SAMPLES = 2000 # @param {type:\"integer\"}\n",
                "SAMPLING_METHOD = \"ddpm\" # @param [\"ddpm\", \"ddim\"]\n",
                "COMPILE_MODE = \"none\" # @param [\"none\", \"default\", \"reduce-overhead\", \"max-autotune\"]\n",
                "DEVICE = \"cuda\" # @param [\"cuda\", \"cpu\"]\n",
                "\n",
                "# --- Evaluation Options ---\n",
                "EXCLUDE_VOLUME = True # @param {type:\"boolean\"}\n",
                "CACHE_SAMPLES_TO_DRIVE = True # @param {type:\"boolean\"}\n",
                "USE_CACHED_SAMPLES = True # @param {type:\"boolean\"}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Setup (Clone, Install, Mount)\n",
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "from google.colab import drive\n",
                "\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_NAME = \"waveletDiff_synth_data\"\n",
                "REPO_PATH = os.path.abspath(REPO_NAME)\n",
                "\n",
                "# 1. Clone or Pull Repo\n",
                "if os.path.exists(REPO_PATH):\n",
                "    nested_path = os.path.join(REPO_PATH, REPO_NAME)\n",
                "    if os.path.exists(nested_path):\n",
                "        shutil.rmtree(REPO_PATH)\n",
                "        !git clone {REPO_URL} {REPO_NAME}\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "    else:\n",
                "        !git -C {REPO_NAME} fetch origin\n",
                "        !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "        !git -C {REPO_NAME} pull origin {REPO_BRANCH}\n",
                "else:\n",
                "    !git clone {REPO_URL} {REPO_NAME}\n",
                "    !git -C {REPO_NAME} checkout {REPO_BRANCH}\n",
                "\n",
                "# 2. Install Dependencies\n",
                "!pip install -q pytorch-lightning pywavelets scipy pandas tqdm scikit-learn tslearn seaborn statsmodels\n",
                "\n",
                "# 3. Mount Drive & Setup Paths\n",
                "if not os.path.exists(DRIVE_MOUNT_PATH):\n",
                "    drive.mount(DRIVE_MOUNT_PATH)\n",
                "\n",
                "for p in [os.path.join(REPO_PATH, \"src\"), os.path.join(REPO_PATH, \"src\", \"evaluation\")]:\n",
                "    if p not in sys.path: sys.path.append(p)\n",
                "\n",
                "# 4. Prepare Experiments\n",
                "local_exp_dir = os.path.join(REPO_PATH, \"outputs\", EXPERIMENT_NAME)\n",
                "os.makedirs(local_exp_dir, exist_ok=True)\n",
                "\n",
                "# (Unpacking logic shortened for brevity - relies on user awareness or existing logic)\n",
                "if os.path.exists(DRIVE_CHECKPOINT_PATH):\n",
                "    print(f\"Unpacking model from {DRIVE_CHECKPOINT_PATH}...\")\n",
                "    if DRIVE_CHECKPOINT_PATH.endswith(\".ckpt\"):\n",
                "        shutil.copy2(DRIVE_CHECKPOINT_PATH, os.path.join(local_exp_dir, \"checkpoint.ckpt\"))\n",
                "    else:\n",
                "        shutil.unpack_archive(DRIVE_CHECKPOINT_PATH, local_exp_dir, format='gztar' if '.gz' in DRIVE_CHECKPOINT_PATH and not '.tar' in DRIVE_CHECKPOINT_PATH else None)\n",
                "else:\n",
                "    print(f\"❌ Model file not found.\")\n",
                "\n",
                "# Sync configs\n",
                "if os.path.exists(os.path.join(REPO_PATH, \"WaveletDiff_source\", \"configs\")):\n",
                "    shutil.rmtree(os.path.join(REPO_PATH, \"configs\"), ignore_errors=True)\n",
                "    shutil.copytree(os.path.join(REPO_PATH, \"WaveletDiff_source\", \"configs\"), os.path.join(REPO_PATH, \"configs\"))\n",
                "\n",
                "print(\"✅ Setup Complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gen_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Generate or Load Samples\n",
                "import numpy as np\n",
                "os.chdir(os.path.join(REPO_PATH, \"src\"))\n",
                "\n",
                "local_gen_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples.npy\"\n",
                "local_real_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples.npy\"\n",
                "drive_gen_path = os.path.join(DRIVE_SAMPLES_PATH, f\"{SAMPLING_METHOD}_samples.npy\")\n",
                "drive_real_path = os.path.join(DRIVE_SAMPLES_PATH, \"real_samples.npy\")\n",
                "\n",
                "if USE_CACHED_SAMPLES and os.path.exists(drive_gen_path):\n",
                "    print(\"Loading cached samples...\")\n",
                "    os.makedirs(os.path.dirname(local_gen_path), exist_ok=True)\n",
                "    shutil.copy2(drive_gen_path, local_gen_path); shutil.copy2(drive_real_path, local_real_path)\n",
                "else:\n",
                "    print(f\"Generating {NUM_SAMPLES} samples...\")\n",
                "    !python sample.py --experiment_name {EXPERIMENT_NAME} --dataset {DATASET} --num_samples {NUM_SAMPLES} --sampling_method {SAMPLING_METHOD} --compile_mode {COMPILE_MODE}\n",
                "    if CACHE_SAMPLES_TO_DRIVE:\n",
                "        os.makedirs(DRIVE_SAMPLES_PATH, exist_ok=True)\n",
                "        shutil.copy2(local_gen_path, drive_gen_path); shutil.copy2(local_real_path, drive_real_path)\n",
                "\n",
                "print(\"✅ Samples Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Initialize Modules\n",
                "import sys\n",
                "import numpy as np\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# New Refactored Modules\n",
                "from evaluation import visualizations as viz\n",
                "from evaluation import statistics as stats\n",
                "from evaluation import reporting as report\n",
                "from evaluation import wrappers\n",
                "\n",
                "# Load Data\n",
                "real_path = f\"../outputs/{EXPERIMENT_NAME}/real_samples.npy\"\n",
                "gen_path = f\"../outputs/{EXPERIMENT_NAME}/{SAMPLING_METHOD}_samples.npy\"\n",
                "real_data = np.load(real_path)\n",
                "generated_data = np.load(gen_path)\n",
                "\n",
                "if EXCLUDE_VOLUME and real_data.shape[2] > 1:\n",
                "    real_data, generated_data = real_data[..., :-1], generated_data[..., :-1]\n",
                "\n",
                "# Downsample\n",
                "n_s = min(2000, len(real_data), len(generated_data))\n",
                "real_data = real_data[np.random.choice(len(real_data), n_s, replace=False)]\n",
                "generated_data = generated_data[np.random.choice(len(generated_data), n_s, replace=False)]\n",
                "\n",
                "# Prepare Raw vs Scaled\n",
                "real_data_raw, generated_data_raw = real_data.copy(), generated_data.copy()\n",
                "\n",
                "dmin, dmax = np.min(real_data, axis=(0,1), keepdims=True), np.max(real_data, axis=(0,1), keepdims=True)\n",
                "real_data_scaled = (real_data - dmin) / (dmax - dmin + 1e-8)\n",
                "generated_data_scaled = (generated_data - dmin) / (dmax - dmin + 1e-8)\n",
                "\n",
                "print(f\"Data Loaded: {real_data.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vis_header",
            "metadata": {},
            "source": [
                "### Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vis_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Visual Analysis\n",
                "viz.plot_distribution_reduction(real_data_scaled, generated_data_scaled)\n",
                "viz.plot_pdf(real_data_scaled, generated_data_scaled)\n",
                "viz.plot_samples(real_data_scaled, generated_data_scaled)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "stats_header",
            "metadata": {},
            "source": [
                "### Statistical & Model Quality Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "metrics_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Run All Metrics\n",
                "metrics_dict = {}\n",
                "\n",
                "# 1. Statistics (Raw Data)\n",
                "stat_agg, real_stat_det, gen_stat_det = stats.calculate_statistical_metrics(real_data_raw, generated_data_raw)\n",
                "metrics_dict['stat_results'] = stat_agg\n",
                "\n",
                "# 2. Discriminative/Predictive (Scaled Data)\n",
                "metrics_dict['discriminative_score'] = wrappers.run_discriminative_benchmark(real_data_scaled, generated_data_scaled)\n",
                "metrics_dict['predictive_score'] = wrappers.run_predictive_benchmark(real_data_scaled, generated_data_scaled)\n",
                "metrics_dict['context_fid_score'] = wrappers.run_context_fid_benchmark(real_data_scaled, generated_data_scaled)\n",
                "\n",
                "# 3. Correlational & DTW\n",
                "metrics_dict['correlational_score'] = wrappers.run_cross_correlation_benchmark(real_data_scaled, generated_data_scaled)\n",
                "metrics_dict['js_results'] = wrappers.run_dtw_benchmark(real_data_scaled, generated_data_scaled)\n",
                "\n",
                "# 4. Advanced Metrics (Raw Data)\n",
                "metrics_dict['dist_results'], metrics_dict['struct_results'], metrics_dict['fin_results'] = wrappers.run_advanced_metrics(real_data_raw, generated_data_raw)\n",
                "metrics_dict['mem_ratio'], metrics_dict['div_results'], metrics_dict['fld_score'] = wrappers.run_new_metrics(real_data_raw, generated_data_raw)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary_header",
            "metadata": {},
            "source": [
                "### Final Scorecard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scorecard_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Summary Scorecard\n",
                "report.generate_summary_scorecard(metrics_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature_stats_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Detailed Feature Stats\n",
                "report.display_feature_stats(real_stat_det, gen_stat_det)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
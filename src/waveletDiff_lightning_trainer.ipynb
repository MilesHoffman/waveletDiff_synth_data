{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fe82947b",
            "metadata": {},
            "source": [
                "# WaveletDiff Lightning Trainer (High Performance)\n",
                "\n",
                "This notebook implements the training loop for WaveletDiff using the high-level **PyTorch Lightning Trainer**. \n",
                "It retains the valid Hardware/Performance optimizations of the native Fabric implementation while offering a more standardized API.\n",
                "\n",
                "### Optimizations Preserved:\n",
                "- **Dynamic Precision**: Automatically selects `bf16-true` (TPU) or `bf16-mixed` (GPU).\n",
                "- **BF16 Pre-casting**: Data is cast to BF16 before the loop *only* on TPUs to maximize throughput.\n",
                "- **Smart Gradient Clipping**: Disabled by default.\n",
                "\n",
                "> **Note**: Designed for TPU `v4-8` or `v5e` but compatible with GPUs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de4542ed",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "# Experiment Identity\n",
                "RUN_NAME = \"lightning_trainer_v1\" # @param {type:\"string\"}\n",
                "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "UNIQUE_RUN_NAME = f\"{RUN_NAME}_{RUN_ID}\"\n",
                "\n",
                "# Dataset Configuration\n",
                "DATASET_NAME = \"stocks\"\n",
                "SEQ_LEN = 24 # @param {type:\"integer\"}\n",
                "\n",
                "# Model Hyperparameters\n",
                "EMBED_DIM = 256 # @param {type:\"integer\"}\n",
                "NUM_HEADS = 8 # @param {type:\"integer\"}\n",
                "NUM_LAYERS = 8 # @param {type:\"integer\"}\n",
                "TIME_EMBED_DIM = 128 # @param {type:\"integer\"}\n",
                "DROPOUT = 0.1 # @param {type:\"number\"}\n",
                "PREDICTION_TARGET = \"noise\"\n",
                "USE_CROSS_LEVEL_ATTENTION = True # @param {type:\"boolean\"}\n",
                "\n",
                "# Training Parameters\n",
                "TOTAL_TRAINING_STEPS = 50000 # @param {type:\"integer\"}\n",
                "BATCH_SIZE = 512 # @param {type:\"integer\"}\n",
                "LEARNING_RATE = 2e-4 # @param {type:\"number\"}\n",
                "WEIGHT_DECAY = 1e-5 # @param {type:\"number\"}\n",
                "WARMUP_STEPS = 500 # @param {type:\"integer\"}\n",
                "ENABLE_GRAD_CLIPPING = False # @param {type:\"boolean\"}\n",
                "\n",
                "# Wavelet Configuration\n",
                "WAVELET_TYPE = \"db2\" # @param {type:\"string\"}\n",
                "WAVELET_LEVELS = \"auto\"\n",
                "\n",
                "# Paths\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/personal_drive/trading\"\n",
                "CHECKPOINT_DIR = f\"{DRIVE_BASE_PATH}/checkpoints/{UNIQUE_RUN_NAME}\"\n",
                "REPO_DIR = \"/content/WaveletDiff\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e77a6f72",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Imports & Environment Setup\n",
                "import sys\n",
                "import os\n",
                "import subprocess\n",
                "import torch\n",
                "import time\n",
                "from pathlib import Path\n",
                "\n",
                "# 1. Performance Polishing: Set Matmul Precision for GPUs (Ampere+/L4/A100)\n",
                "if torch.cuda.is_available():\n",
                "    # 'high' uses TensorFloat-32 (TF32) for significantly faster matmuls on L4/A100\n",
                "    torch.set_float32_matmul_precision('high')\n",
                "\n",
                "# 2. Conditional Dependency Installation\n",
                "deps = [\"lightning\", \"pywavelets\", \"scipy\", \"pandas\", \"tqdm\"]\n",
                "\n",
                "# STRICT CHECK: Only install torch_xla if explicitly on a TPU environment\n",
                "is_tpu = 'COLAB_TPU_ADDR' in os.environ or 'TPU_NAME' in os.environ\n",
                "if is_tpu and not any(\"torch_xla\" in line for line in subprocess.getoutput(\"pip list\").splitlines()):\n",
                "    deps.append(\"torch_xla[tpu]\")\n",
                "\n",
                "try:\n",
                "    import lightning.pytorch as pl\n",
                "except ImportError:\n",
                "    print(f\"Installing dependencies ({', '.join(deps)})...\")\n",
                "    subprocess.run([\"pip\", \"install\"] + deps, check=True, stdout=subprocess.DEVNULL)\n",
                "    import lightning.pytorch as pl\n",
                "\n",
                "# 3. Dynamic Precision Detection\n",
                "# Trainer accepts 'bf16-mixed' or 'bf16-true' via the 'precision' arg\n",
                "if is_tpu:\n",
                "    PRECISION = \"bf16-true\"\n",
                "elif torch.cuda.is_available():\n",
                "    PRECISION = \"bf16-mixed\"\n",
                "else:\n",
                "    PRECISION = \"32-true\"\n",
                "\n",
                "# 4. Clone Repository\n",
                "REPO_URL = \"https://github.com/GarlicWang/WaveletDiff.git\"\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    print(f\"Cloning {REPO_URL}...\")\n",
                "    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True, stdout=subprocess.DEVNULL)\n",
                "\n",
                "# 5. Add to System Path\n",
                "sys.path.append(os.path.join(REPO_DIR, \"src\"))\n",
                "\n",
                "# 6. Create Checkpoint Directory\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Environment Ready. Accelerator: {'TPU' if is_tpu else 'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
                "print(f\"Selected Precision: {PRECISION}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0fbaaf2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Data Loading & Optimization\n",
                "import pandas as pd\n",
                "from data.loaders import create_sliding_windows\n",
                "from data.module import WaveletTimeSeriesDataModule\n",
                "\n",
                "def prepare_optimized_datamodule():\n",
                "    \"\"\"Preloads data, creates windows, and optimizes type for HW\"\"\"\n",
                "    stocks_path = os.path.join(REPO_DIR, \"data\", \"stocks\", \"stock_data.csv\")\n",
                "    print(f\"Loading data from {stocks_path}...\")\n",
                "        \n",
                "    df = pd.read_csv(stocks_path)\n",
                "    CORE_COLS = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
                "    df_filtered = df[CORE_COLS]\n",
                "\n",
                "    # Create windows (Memory intensive, so we do it on CPU)\n",
                "    custom_data_windows, _ = create_sliding_windows(\n",
                "        df_filtered.values,\n",
                "        seq_len=SEQ_LEN,\n",
                "        normalize=True\n",
                "    )\n",
                "\n",
                "    full_data_tensor = torch.FloatTensor(custom_data_windows)\n",
                "    \n",
                "    # CONFIG\n",
                "    full_config = {\n",
                "        'dataset': {'name': DATASET_NAME, 'seq_len': SEQ_LEN},\n",
                "        'training': {'batch_size': BATCH_SIZE, 'epochs': 1},\n",
                "        'data': {'data_dir': os.path.join(REPO_DIR, \"data\"), 'normalize_data': False},\n",
                "        'wavelet': {'type': WAVELET_TYPE, 'levels': WAVELET_LEVELS}\n",
                "    }\n",
                "\n",
                "    # Create DataModule (Handles Wavelet Transforms internally during init)\n",
                "    # We pass the tensor explicitly so we don't reload inside.\n",
                "    datamodule = WaveletTimeSeriesDataModule(config=full_config, data_tensor=full_data_tensor)\n",
                "    \n",
                "    # HARDWARE OPTIMIZATION\n",
                "    # We directly access the underlying tensor in the datamodule and cast it if needed.\n",
                "    # 1. Capture the original tensor\n",
                "    data_tensor = datamodule.data_tensor\n",
                "    \n",
                "    # 2. Apply Casting Logic (Match Logic from Fabric Notebook)\n",
                "    # If using bf16-true (TPU), we cast to bf16.\n",
                "    # If using bf16-mixed (GPU), we keep as float32.\n",
                "    if PRECISION == \"bf16-true\" and torch.device(\"cpu\").type != \"cpu\": # Simple logic check, actually relies on is_tpu flag ideally\n",
                "         # Better check: relies on the Global Configuration logic\n",
                "         print(f\"Optimizing: Casting data to bfloat16 for TPU execution...\")\n",
                "         data_tensor = data_tensor.to(torch.bfloat16)\n",
                "    \n",
                "    # 3. Re-assign to datamodule's dataset\n",
                "    from torch.utils.data import TensorDataset\n",
                "    datamodule.data_tensor = data_tensor\n",
                "    datamodule.dataset = TensorDataset(data_tensor)\n",
                "    \n",
                "    return datamodule, full_config\n",
                "\n",
                "datamodule, model_base_config = prepare_optimized_datamodule()\n",
                "WAVELET_INFO = datamodule.get_wavelet_info()\n",
                "INPUT_DIM = datamodule.get_input_dim()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a025d74",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Model Initialization\n",
                "from models.transformer import WaveletDiffusionTransformer\n",
                "\n",
                "# Update Config\n",
                "model_base_config.update({\n",
                "    'model': {\n",
                "        'embed_dim': EMBED_DIM,\n",
                "        'num_heads': NUM_HEADS,\n",
                "        'num_layers': NUM_LAYERS,\n",
                "        'time_embed_dim': TIME_EMBED_DIM,\n",
                "        'dropout': DROPOUT,\n",
                "        'prediction_target': PREDICTION_TARGET\n",
                "    },\n",
                "    'attention': {'use_cross_level_attention': USE_CROSS_LEVEL_ATTENTION},\n",
                "    'noise': {'schedule': \"exponential\"},\n",
                "    'sampling': {'ddim_eta': 0.0, 'ddim_steps': None},\n",
                "    'energy': {'weight': 0.0},\n",
                "    'optimizer': {\n",
                "        'scheduler_type': 'onecycle',\n",
                "        'lr': LEARNING_RATE,\n",
                "        'warmup_epochs': 5,\n",
                "        'cosine_eta_min': 1e-6\n",
                "    }\n",
                "})\n",
                "\n",
                "model = WaveletDiffusionTransformer(data_module=datamodule, config=model_base_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0f04a65e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 5: Lightning Trainer Setup & Training\n",
                "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
                "from lightning.pytorch.loggers import CSVLogger\n",
                "\n",
                "# Callbacks\n",
                "checkpoint_callback = ModelCheckpoint(\n",
                "    dirpath=CHECKPOINT_DIR,\n",
                "    filename='{step}-{train_loss:.4f}',\n",
                "    save_top_k=-1, # Save latest, or specific frequency\n",
                "    every_n_train_steps=5000, # Matches SAVE_INTERVAL\n",
                "    save_last=True\n",
                ")\n",
                "\n",
                "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
                "\n",
                "# Trainer\n",
                "trainer = pl.Trainer(\n",
                "    accelerator=\"auto\",\n",
                "    devices=\"auto\",\n",
                "    precision=PRECISION,\n",
                "    max_steps=TOTAL_TRAINING_STEPS,\n",
                "    logger=CSVLogger(save_dir=DRIVE_BASE_PATH, name=UNIQUE_RUN_NAME),\n",
                "    callbacks=[checkpoint_callback, lr_monitor],\n",
                "    enable_checkpointing=True,\n",
                "    enable_progress_bar=True,\n",
                "    gradient_clip_val=1.0 if ENABLE_GRAD_CLIPPING else None,\n",
                "    log_every_n_steps=100 # Default moderate logging\n",
                ")\n",
                "\n",
                "# Start Training\n",
                "print(\"Starting Training via pl.Trainer...\")\n",
                "trainer.fit(model, datamodule=datamodule)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f615614a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 6: Evaluation (Sanity Check)\n",
                "\n",
                "def sanity_check_sampling():\n",
                "    # Move model to evaluation on appropriate device\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'xla' if is_tpu else 'cpu')\n",
                "    if is_tpu:\n",
                "       import torch_xla.core.xla_model as xm\n",
                "       device = xm.xla_device()\n",
                "    \n",
                "    model.to(device)\n",
                "    model.eval()\n",
                "    \n",
                "    num_samples = 2\n",
                "    print(\"Generating sanity check samples...\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        # Start from random noise in wavelet domain\n",
                "        shape = (num_samples, INPUT_DIM, WAVELET_INFO['n_features'])\n",
                "        # Match dtype to model dtype (bf16 or float32)\n",
                "        dtype = torch.bfloat16 if PRECISION.startswith('bf16') else torch.float32\n",
                "        \n",
                "        samples_wavelet = torch.randn(shape, device=device, dtype=dtype)\n",
                "        \n",
                "        # Reverse diffusion\n",
                "        from tqdm.auto import tqdm\n",
                "        for i in tqdm(reversed(range(model.T)), total=model.T, desc=\"Sampling\"):\n",
                "            t = torch.full((num_samples,), i, device=device, dtype=torch.long)\n",
                "            t_norm = t.float() / model.T\n",
                "            \n",
                "            eps_theta = model(samples_wavelet, t_norm)\n",
                "            \n",
                "            # Standard DDPM update (simplified)\n",
                "            alpha_t = model.alpha_all[t].view(-1, 1, 1)\n",
                "            alpha_bar_t = model.alpha_bar_all[t].view(-1, 1, 1)\n",
                "            beta_t = model.beta_all[t].view(-1, 1, 1)\n",
                "            \n",
                "            mean = (1 / torch.sqrt(alpha_t)) * (\n",
                "                samples_wavelet - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * eps_theta\n",
                "            )\n",
                "            \n",
                "            if i > 0:\n",
                "                noise = torch.randn_like(samples_wavelet)\n",
                "                samples_wavelet = mean + torch.sqrt(beta_t) * noise\n",
                "            else:\n",
                "                samples_wavelet = mean\n",
                "\n",
                "    print(\"Sampling complete. Converting to time series...\")\n",
                "    # Convert back to time series (ensure cpu float32 for reconstruction stability)\n",
                "    samples_wavelet_cpu = samples_wavelet.float().cpu()\n",
                "    samples_ts = datamodule.convert_wavelet_to_timeseries(samples_wavelet_cpu)\n",
                "    print(f\"Generated samples shape: {samples_ts.shape}\")\n",
                "    return samples_ts\n",
                "\n",
                "samples = sanity_check_sampling()"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
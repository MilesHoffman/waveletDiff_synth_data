{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "639c738b",
            "metadata": {},
            "source": [
                "# WaveletDiff Fabric Training (Frontend Notebook)\n",
                "\n",
                "This notebook serves as the frontend for the WaveletDiff training pipeline. The heavy lifting is handled by `src.torch_gpu_waveletDiff.train.trainer`.\n",
                "\n",
                "### Workflow:\n",
                "1. **Configuration**: Set your parameters here.\n",
                "2. **Setup**: Clones the repo and installs dependencies.\n",
                "3. **Initalization**: Sets up Fabric, Model, and Data via the backend.\n",
                "4. **Execution**: Runs the high-performance training loop.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5be5e355",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import random\n",
                "\n",
                "# Experiment Identity\n",
                "RUN_NAME = \"stocks_ohlcv_v1\" # @param {type:\"string\"}\n",
                "RUN_ID =  f\"{random.randint(0, 999):03d}\"\n",
                "UNIQUE_RUN_NAME = f\"{RUN_NAME}_{RUN_ID}\"\n",
                "\n",
                "# Dataset Configuration\n",
                "DATASET_NAME = \"stocks\"\n",
                "DATA_PATH = \"src/copied_waveletDiff/data/stocks/stock_data.csv\" # @param {type:\"string\"}\n",
                "SEQ_LEN = 24 # @param {type:\"integer\"}\n",
                "\n",
                "# Model Hyperparameters\n",
                "EMBED_DIM = 256 # @param {type:\"integer\"}\n",
                "NUM_HEADS = 8 # @param {type:\"integer\"}\n",
                "NUM_LAYERS = 8 # @param {type:\"integer\"}\n",
                "TIME_EMBED_DIM = 128 # @param {type:\"integer\"}\n",
                "DROPOUT = 0.1 # @param {type:\"number\"}\n",
                "PREDICTION_TARGET = \"noise\"\n",
                "USE_CROSS_LEVEL_ATTENTION = True # @param {type:\"boolean\"}\n",
                "\n",
                "# Training Parameters\n",
                "TOTAL_TRAINING_STEPS = 35000 # @param {type:\"integer\"}\n",
                "BATCH_SIZE = 512 # @param {type:\"integer\"}\n",
                "LEARNING_RATE = 2e-4 # @param {type:\"number\"}\n",
                "WEIGHT_DECAY = 1e-5 # @param {type:\"number\"}\n",
                "ENABLE_GRAD_CLIPPING = True # @param {type:\"boolean\"}\n",
                "# ^ CRITICAL: Enabled for stability (prevents exploding gradients). Small perf check on TPU.\n",
                "\n",
                "# Precision\n",
                "PRECISION = \"bf16-mixed\" # @param [\"bf16-mixed\", \"32-true\"]\n",
                "MATMUL_PRECISION = \"high\" # @param [\"medium\", \"high\"]\n",
                "\n",
                "# Profiler\n",
                "ENABLE_PROFILER = False # @param {type:\"boolean\"}\n",
                "\n",
                "# Optimizer Configuration\n",
                "# Matched to source repo defaults for OneCycleLR\n",
                "MAX_LR = 1e-3 # @param {type:\"number\"}\n",
                "PCT_START = 0.3 # @param {type:\"number\"}\n",
                "\n",
                "# Checkpointing\n",
                "SAVE_INTERVAL = 5000 # @param {type:\"integer\"}\n",
                "\n",
                "# Wavelet Configuration\n",
                "WAVELET_TYPE = \"db2\" # @param {type:\"string\"}\n",
                "WAVELET_LEVELS = \"auto\"\n",
                "\n",
                "# Paths\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/personal_drive/trading\"\n",
                "CHECKPOINT_DIR = f\"{DRIVE_BASE_PATH}/checkpoints/{UNIQUE_RUN_NAME}\"\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_DIR = \"/content/waveletDiff_synth_data\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d31d0907",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Setup (Clone, Install, Mount)\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "import shutil\n",
                "\n",
                "# 1. Mount Drive\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    if os.path.exists('/content/drive'):\n",
                "        if not os.listdir('/content/drive'):\n",
                "            print(\"Detected ghost directory. Force remounting...\")\n",
                "            drive.mount('/content/drive', force_remount=True)\n",
                "    else:\n",
                "        drive.mount('/content/drive')\n",
                "except ImportError:\n",
                "    print(\"Not running on Colab. Skipping Drive mount.\")\n",
                "\n",
                "# 2. Force Clone Repository\n",
                "if os.path.exists(REPO_DIR):\n",
                "    print(f\"Repo exists at {REPO_DIR}, pulling changes...\")\n",
                "    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\"], check=True)\n",
                "else:\n",
                "    print(f\"Cloning {REPO_URL} into {REPO_DIR}...\")\n",
                "    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
                "\n",
                "# 3. Install Dependencies\n",
                "print(\"Installing dependencies (this may take a minute)...\")\n",
                "deps = [\"lightning\", \"pywavelets\", \"scipy\", \"pandas\", \"tqdm\"]\n",
                "is_tpu = 'COLAB_TPU_ADDR' in os.environ or 'TPU_NAME' in os.environ\n",
                "if is_tpu and not any(\"torch_xla\" in line for line in subprocess.getoutput(\"pip list\").splitlines()):\n",
                "    deps.append(\"torch_xla[tpu]\")\n",
                "\n",
                "try:\n",
                "    import lightning.fabric\n",
                "except ImportError:\n",
                "    subprocess.run([\"pip\", \"install\"] + deps, check=True)\n",
                "\n",
                "# 4. Setup Paths\n",
                "# Critical: We add the repo to path so we can import src.torch_gpu_waveletDiff\n",
                "if REPO_DIR not in sys.path:\n",
                "    sys.path.append(REPO_DIR)\n",
                "# Also add the inner source for legacy imports inside the trainer\n",
                "source_path = os.path.join(REPO_DIR, \"src\", \"copied_waveletDiff\", \"src\")\n",
                "if source_path not in sys.path:\n",
                "    sys.path.append(source_path)\n",
                "\n",
                "print(\"Setup Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4169bed8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Initialize Fabric\n",
                "from src.torch_gpu_waveletDiff.train import trainer\n",
                "\n",
                "fabric = trainer.setup_fabric(precision=PRECISION, matmul_precision=MATMUL_PRECISION)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0296d61c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Load Data\n",
                "train_loader, datamodule, config = trainer.get_dataloaders(\n",
                "    fabric=fabric,\n",
                "    repo_dir=REPO_DIR,\n",
                "    dataset_name=DATASET_NAME,\n",
                "    seq_len=SEQ_LEN,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    wavelet_type=WAVELET_TYPE,\n",
                "    wavelet_levels=WAVELET_LEVELS,\n",
                "    data_path=DATA_PATH\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "306e5338",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 5: Initialize Model\n",
                "model, optimizer, config = trainer.init_model(\n",
                "    fabric=fabric,\n",
                "    datamodule=datamodule,\n",
                "    config=config,\n",
                "    embed_dim=EMBED_DIM,\n",
                "    num_heads=NUM_HEADS,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    time_embed_dim=TIME_EMBED_DIM,\n",
                "    dropout=DROPOUT,\n",
                "    prediction_target=PREDICTION_TARGET,\n",
                "    use_cross_level_attention=USE_CROSS_LEVEL_ATTENTION,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    weight_decay=WEIGHT_DECAY,\n",
                "    max_lr=MAX_LR,\n",
                "    pct_start=PCT_START\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b6924e91",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 6: Run Training\n",
                "trainer.train_loop(\n",
                "    fabric=fabric,\n",
                "    model=model,\n",
                "    optimizer=optimizer,\n",
                "    train_loader=train_loader,\n",
                "    config=config,\n",
                "    total_steps=TOTAL_TRAINING_STEPS,\n",
                "    save_interval=SAVE_INTERVAL,\n",
                "    checkpoint_dir=CHECKPOINT_DIR,\n",
                "    enable_profiler=ENABLE_PROFILER,\n",
                "    enable_grad_clipping=ENABLE_GRAD_CLIPPING\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "9a17bf3e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Global Configuration\n",
                "# Hyperparameters and Paths\n",
                "BATCH_SIZE = 32\n",
                "LEARNING_RATE = 1e-4\n",
                "DEVICE_OVERRIDE = None  # Force a device: \"cuda\", \"mps\", or \"cpu\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b72bc25",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ NVIDIA GPU Detected: Tesla T4\n",
                        "   - Total VRAM: 14.74 GB\n",
                        "--- Environment Summary ---\n",
                        "PyTorch Version: 2.9.0+cu126\n",
                        "Python Version: 3.12.12\n",
                        "System: Linux 6.6.105+\n",
                        "Active Device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# @title Imports & Hardware Setup\n",
                "import torch\n",
                "import platform\n",
                "import os\n",
                "import sys\n",
                "\n",
                "def get_optimal_device():\n",
                "    \"\"\"\n",
                "    Dynamically checks for hardware resources and returns the optimal device.\n",
                "    Displays specific GPU model (T4, L4, A100, etc.) and VRAM metrics.\n",
                "    \"\"\"\n",
                "    if torch.cuda.is_available():\n",
                "        device_name = torch.cuda.get_device_name(0)\n",
                "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
                "        print(f\"✅ NVIDIA GPU Detected: {device_name}\")\n",
                "        print(f\"   - Total VRAM: {total_memory:.2f} GB\")\n",
                "        return torch.device(\"cuda\")\n",
                "    \n",
                "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
                "        print(\"✅ Apple Silicon (MPS) Detected.\")\n",
                "        return torch.device(\"mps\")\n",
                "    \n",
                "    print(\"⚠️ No hardware acceleration found. Defaulting to CPU.\")\n",
                "    return torch.device(\"cpu\")\n",
                "\n",
                "# Initialize device\n",
                "device = torch.device(DEVICE_OVERRIDE) if DEVICE_OVERRIDE else get_optimal_device()\n",
                "\n",
                "print(f\"--- Environment Summary ---\")\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"Python Version: {platform.python_version()}\")\n",
                "print(f\"System: {platform.system()} {platform.release()}\")\n",
                "print(f\"Active Device: {device}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

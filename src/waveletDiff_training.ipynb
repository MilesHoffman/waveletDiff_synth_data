{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "639c738b",
            "metadata": {},
            "source": [
                "# WaveletDiff Training (PyTorch Lightning)\n",
                "\n",
                "This notebook uses PyTorch Lightning Trainer (matching the source repo exactly).\n",
                "\n",
                "### Workflow:\n",
                "1. **Configuration**: Set your parameters here.\n",
                "2. **Setup**: Clones the repo and installs dependencies.\n",
                "3. **Data**: Load and prepare data via DataModule.\n",
                "4. **Model**: Initialize the WaveletDiffusionTransformer.\n",
                "5. **Train**: Run training with pl.Trainer.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5be5e355",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import random\n",
                "\n",
                "# Experiment Identity\n",
                "RUN_NAME = \"stocks_baseline_v1\" # @param {type:\"string\"}\n",
                "RUN_ID =  f\"{random.randint(0, 999):03d}\"\n",
                "UNIQUE_RUN_NAME = f\"{RUN_NAME}_{RUN_ID}\"\n",
                "\n",
                "# Dataset Configuration\n",
                "DATASET_NAME = \"stocks\"\n",
                "DATA_PATH = \"src/copied_waveletDiff/data/stocks/stock_data.csv\" # @param {type:\"string\"}\n",
                "SEQ_LEN = 24 # @param {type:\"integer\"}\n",
                "\n",
                "# Model Hyperparameters\n",
                "EMBED_DIM = 256 # @param {type:\"integer\"}\n",
                "NUM_HEADS = 8 # @param {type:\"integer\"}\n",
                "NUM_LAYERS = 8 # @param {type:\"integer\"}\n",
                "TIME_EMBED_DIM = 128 # @param {type:\"integer\"}\n",
                "DROPOUT = 0.1 # @param {type:\"number\"}\n",
                "PREDICTION_TARGET = \"noise\"\n",
                "USE_CROSS_LEVEL_ATTENTION = True # @param {type:\"boolean\"}\n",
                "\n",
                "# Training Parameters\n",
                "NUM_EPOCHS = 100 # @param {type:\"integer\"}\n",
                "BATCH_SIZE = 512 # @param {type:\"integer\"}\n",
                "LEARNING_RATE = 2e-4 # @param {type:\"number\"}\n",
                "GRADIENT_CLIP_VAL = 1.0 # @param {type:\"number\"}\n",
                "\n",
                "# Precision (matches source: \"32\" for FP32)\n",
                "PRECISION = \"32\" # @param [\"32\", \"bf16-mixed\"]\n",
                "MATMUL_PRECISION = \"medium\" # @param [\"medium\", \"high\"]\n",
                "\n",
                "# torch.compile (optional, set to None to disable)\n",
                "COMPILE_MODE = None # @param [\"None\", \"default\", \"reduce-overhead\", \"max-autotune\"]\n",
                "COMPILE_FULLGRAPH = False # @param {type:\"boolean\"}\n",
                "\n",
                "# Logging\n",
                "LOG_EVERY_N_STEPS = 50 # @param {type:\"integer\"}\n",
                "LOG_EVERY_N_EPOCHS = 1 # @param {type:\"integer\"}\n",
                "ENABLE_PROGRESS_BAR = True # @param {type:\"boolean\"}\n",
                "\n",
                "# Checkpointing\n",
                "SAVE_EVERY_N_EPOCHS = 100 # @param {type:\"integer\"}\n",
                "\n",
                "# Wavelet Configuration\n",
                "WAVELET_TYPE = \"db2\" # @param {type:\"string\"}\n",
                "WAVELET_LEVELS = \"auto\"\n",
                "\n",
                "# Paths\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/personal_drive/trading\"\n",
                "CHECKPOINT_DIR = f\"{DRIVE_BASE_PATH}/checkpoints/temp/{UNIQUE_RUN_NAME}\"\n",
                "COMPILE_CACHE_DIR = f\"{DRIVE_BASE_PATH}/compile_cache\"\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_DIR = \"/content/waveletDiff_synth_data\"\n",
                "\n",
                "# Reproducibility Seed (set to None for random)\n",
                "SEED = 42 # @param {type:\"integer\"}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d31d0907",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Setup (Clone, Install, Mount)\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "# 1. Mount Drive\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    if os.path.exists('/content/drive'):\n",
                "        if not os.listdir('/content/drive'):\n",
                "            print(\"Force remounting Drive...\")\n",
                "            drive.mount('/content/drive', force_remount=True)\n",
                "    else:\n",
                "        drive.mount('/content/drive')\n",
                "    print(\"✅ Drive mounted\")\n",
                "except ImportError:\n",
                "    print(\"Not running on Colab. Skipping Drive mount.\")\n",
                "    DRIVE_BASE_PATH = \"local_checkpoints\"\n",
                "    os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
                "    COMPILE_CACHE_DIR = \"local_compile_cache\"\n",
                "\n",
                "# 2. Clone/Update Repo\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    print(f\"Cloning {REPO_URL} into {REPO_DIR}...\")\n",
                "    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
                "else:\n",
                "    print(f\"Repo exists at {REPO_DIR}, pulling latest changes...\")\n",
                "    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\"], check=True)\n",
                "\n",
                "# 3. Add to Path\n",
                "if REPO_DIR not in sys.path:\n",
                "    sys.path.insert(0, REPO_DIR)\n",
                "\n",
                "# 4. Install Dependencies\n",
                "print(\"Installing dependencies...\")\n",
                "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pytorch-lightning\", \"pywavelets\", \"scipy\", \"pandas\", \"tqdm\", \"lightning\"], check=True)\n",
                "print(\"✅ Dependencies installed\")\n",
                "\n",
                "import importlib\n",
                "importlib.invalidate_caches()\n",
                "print(\"✅ Env Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4169bed8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Setup Environment\n",
                "from src.torch_gpu_waveletDiff.train import trainer\n",
                "\n",
                "trainer.setup_environment(matmul_precision=MATMUL_PRECISION, seed=SEED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0296d61c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Load Data\n",
                "datamodule, config = trainer.get_datamodule(\n",
                "    repo_dir=REPO_DIR,\n",
                "    dataset_name=DATASET_NAME,\n",
                "    seq_len=SEQ_LEN,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    wavelet_type=WAVELET_TYPE,\n",
                "    wavelet_levels=WAVELET_LEVELS,\n",
                "    data_path=DATA_PATH\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "306e5338",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 5: Initialize Model\n",
                "model, config = trainer.init_model(\n",
                "    datamodule=datamodule,\n",
                "    config=config,\n",
                "    embed_dim=EMBED_DIM,\n",
                "    num_heads=NUM_HEADS,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    time_embed_dim=TIME_EMBED_DIM,\n",
                "    dropout=DROPOUT,\n",
                "    prediction_target=PREDICTION_TARGET,\n",
                "    use_cross_level_attention=USE_CROSS_LEVEL_ATTENTION,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    compile_mode=COMPILE_MODE if COMPILE_MODE != \"None\" else None,\n",
                "    compile_fullgraph=COMPILE_FULLGRAPH,\n",
                "    compile_cache_dir=COMPILE_CACHE_DIR\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b6924e91",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 6: Run Training\n",
                "pl_trainer, trained_model = trainer.train(\n",
                "    model=model,\n",
                "    datamodule=datamodule,\n",
                "    config=config,\n",
                "    num_epochs=NUM_EPOCHS,\n",
                "    precision=PRECISION,\n",
                "    gradient_clip_val=GRADIENT_CLIP_VAL,\n",
                "    log_every_n_steps=LOG_EVERY_N_STEPS,\n",
                "    log_every_n_epochs=LOG_EVERY_N_EPOCHS,\n",
                "    checkpoint_dir=CHECKPOINT_DIR,\n",
                "    save_every_n_epochs=SAVE_EVERY_N_EPOCHS,\n",
                "    enable_progress_bar=ENABLE_PROGRESS_BAR\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0326e7a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 7: Persist Compilation Cache\n",
                "# Run this cell after training (or after the first few steps) \n",
                "# to save your optimized kernels to Google Drive as a single archive.\n",
                "trainer.persist_compilation_cache(COMPILE_CACHE_DIR)"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
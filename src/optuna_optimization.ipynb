{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# WaveletDiff Optuna Hyperparameter Optimization\n",
                "\n",
                "Modern hyperparameter optimization with:\n",
                "- üéØ Multi-objective optimization (loss, speed, stability)\n",
                "- üß† TPESampler for intelligent search\n",
                "- ‚úÇÔ∏è HyperbandPruner for early stopping\n",
                "- üíæ Persistent SQLite storage (survives Colab restarts)\n",
                "- üìä Optuna Dashboard for visualization\n",
                "\n",
                "### Workflow:\n",
                "1. Configure which hyperparameters to tune (Cell 1)\n",
                "2. Setup environment and mount Drive (Cell 2)\n",
                "3. Initialize Fabric and data (Cell 3)\n",
                "4. Create Optuna study (Cell 4)\n",
                "5. Launch dashboard (Cell 5) - optional\n",
                "6. Run optimization (Cell 6)\n",
                "7. Analyze results (Cell 7)\n",
                "8. Export best configs (Cell 8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Hyperparameter Tuning Configuration\n",
                "\n",
                "# Study Settings\n",
                "STUDY_NAME = \"waveletdiff_multiobjective_v1\"  # @param {type:\"string\"}\n",
                "N_TRIALS = 50  # @param {type:\"integer\"}\n",
                "TIMEOUT_HOURS = None  # @param {type:\"number\"}\n",
                "\n",
                "# Trial Settings\n",
                "STEPS_PER_TRIAL = 2000  # @param {type:\"integer\"}\n",
                "EVAL_INTERVAL = 100  # @param {type:\"integer\"}\n",
                "\n",
                "# Optimization Mode\n",
                "USE_MULTI_OBJECTIVE = True  # @param {type:\"boolean\"}\n",
                "# ^ If False, uses weighted scalarization (single objective)\n",
                "\n",
                "# Multi-Objective Weights (only used if USE_MULTI_OBJECTIVE=False)\n",
                "WEIGHT_LOSS = 1.0  # @param {type:\"number\"}\n",
                "WEIGHT_SPEED = 0.001  # @param {type:\"number\"}\n",
                "WEIGHT_STABILITY = 0.2  # @param {type:\"number\"}\n",
                "\n",
                "# Pruner Settings\n",
                "ENABLE_PRUNING = True  # @param {type:\"boolean\"}\n",
                "PRUNER_TYPE = \"hyperband\"  # @param [\"hyperband\", \"median\", \"none\"]\n",
                "PRUNER_MIN_RESOURCE = 500  # @param {type:\"integer\"}\n",
                "PRUNER_REDUCTION_FACTOR = 3  # @param {type:\"integer\"}\n",
                "\n",
                "# Sampler Settings\n",
                "SAMPLER_TYPE = \"tpe\"  # @param [\"tpe\", \"random\"]\n",
                "N_STARTUP_TRIALS = 10  # @param {type:\"integer\"}\n",
                "\n",
                "# Dashboard\n",
                "ENABLE_DASHBOARD = True  # @param {type:\"boolean\"}\n",
                "DASHBOARD_PORT = 8080  # @param {type:\"integer\"}\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"HYPERPARAMETERS TO TUNE\")\n",
                "print(\"=\"*60)\n",
                "print(\"Toggle each parameter ON (True) or OFF (False)\\n\")\n",
                "\n",
                "# Hyperparameter Tuning Toggles\n",
                "TUNE_LEARNING_RATE = True  # @param {type:\"boolean\"}\n",
                "TUNE_MAX_LR = True  # @param {type:\"boolean\"}\n",
                "TUNE_WEIGHT_DECAY = True  # @param {type:\"boolean\"}\n",
                "TUNE_EMBED_DIM = True  # @param {type:\"boolean\"}\n",
                "TUNE_NUM_HEADS = False  # @param {type:\"boolean\"}\n",
                "TUNE_NUM_LAYERS = True  # @param {type:\"boolean\"}\n",
                "TUNE_DROPOUT = True  # @param {type:\"boolean\"}\n",
                "TUNE_BATCH_SIZE = True  # @param {type:\"boolean\"}\n",
                "TUNE_PCT_START = False  # @param {type:\"boolean\"}\n",
                "TUNE_GRAD_CLIP_NORM = False  # @param {type:\"boolean\"}\n",
                "TUNE_TIME_EMBED_DIM = False  # @param {type:\"boolean\"}\n",
                "\n",
                "# Collect tune flags\n",
                "TUNE_FLAGS = {\n",
                "    'learning_rate': TUNE_LEARNING_RATE,\n",
                "    'max_lr': TUNE_MAX_LR,\n",
                "    'weight_decay': TUNE_WEIGHT_DECAY,\n",
                "    'embed_dim': TUNE_EMBED_DIM,\n",
                "    'num_heads': TUNE_NUM_HEADS,\n",
                "    'num_layers': TUNE_NUM_LAYERS,\n",
                "    'dropout': TUNE_DROPOUT,\n",
                "    'batch_size': TUNE_BATCH_SIZE,\n",
                "    'pct_start': TUNE_PCT_START,\n",
                "    'grad_clip_norm': TUNE_GRAD_CLIP_NORM,\n",
                "    'time_embed_dim': TUNE_TIME_EMBED_DIM,\n",
                "}\n",
                "\n",
                "# Default Hyperparameters (used when tuning is disabled)\n",
                "DEFAULT_HYPERPARAMS = {\n",
                "    'learning_rate': 2e-4,\n",
                "    'max_lr': 1e-3,\n",
                "    'weight_decay': 1e-5,\n",
                "    'embed_dim': 256,\n",
                "    'num_heads': 8,\n",
                "    'num_layers': 8,\n",
                "    'dropout': 0.1,\n",
                "    'batch_size': 512,\n",
                "    'pct_start': 0.3,\n",
                "    'grad_clip_norm': 1.0,\n",
                "    'time_embed_dim': 128,\n",
                "}\n",
                "\n",
                "# Dataset Configuration\n",
                "DATASET_NAME = \"stocks\"  # @param {type:\"string\"}\n",
                "SEQ_LEN = 24  # @param {type:\"integer\"}\n",
                "WAVELET_TYPE = \"db2\"  # @param {type:\"string\"}\n",
                "WAVELET_LEVELS = \"auto\"\n",
                "DATA_PATH = \"src/copied_waveletDiff/data/stocks/stock_data.csv\"  # @param {type:\"string\"}\n",
                "\n",
                "# Paths\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/personal_drive/trading\"  # @param {type:\"string\"}\n",
                "OPTUNA_DB_PATH = f\"{DRIVE_BASE_PATH}/optuna_studies/waveletdiff.db\"\n",
                "CHECKPOINT_DIR = f\"{DRIVE_BASE_PATH}/optuna_checkpoints/temp\"\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_DIR = \"/content/waveletDiff_synth_data\"\n",
                "\n",
                "# Print summary\n",
                "tuned_params = [k for k, v in TUNE_FLAGS.items() if v]\n",
                "fixed_params = [k for k, v in TUNE_FLAGS.items() if not v]\n",
                "\n",
                "print(f\"Tuning {len(tuned_params)} parameters:\")\n",
                "for param in tuned_params:\n",
                "    print(f\"  ‚úÖ {param}\")\n",
                "print(f\"\\nFixed {len(fixed_params)} parameters:\")\n",
                "for param in fixed_params:\n",
                "    print(f\"  ‚õî {param}: {DEFAULT_HYPERPARAMS[param]}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Environment Setup\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "# Mount Drive\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    if os.path.exists('/content/drive'):\n",
                "        if not os.listdir('/content/drive'):\n",
                "            print(\"Force remounting Drive...\")\n",
                "            drive.mount('/content/drive', force_remount=True)\n",
                "    else:\n",
                "        drive.mount('/content/drive')\n",
                "    print(\"‚úÖ Drive mounted\")\n",
                "except ImportError:\n",
                "    print(\"Not running on Colab. Skipping Drive mount.\")\n",
                "\n",
                "# Clone Repository\n",
                "if os.path.exists(REPO_DIR):\n",
                "    print(f\"Repo exists at {REPO_DIR}, pulling changes...\")\n",
                "    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\"], check=True)\n",
                "else:\n",
                "    print(f\"Cloning {REPO_URL} into {REPO_DIR}...\")\n",
                "    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
                "\n",
                "print(\"‚úÖ Repository ready\")\n",
                "\n",
                "# Install Dependencies\n",
                "print(\"Installing dependencies...\")\n",
                "deps = [\"lightning\", \"pywavelets\", \"scipy\", \"pandas\", \"tqdm\", \"optuna\", \"optuna-dashboard\", \"plotly\", \"kaleido\"]\n",
                "subprocess.run([\"pip\", \"install\", \"-q\"] + deps, check=True)\n",
                "print(\"‚úÖ Dependencies installed\")\n",
                "\n",
                "# Setup Paths\n",
                "if REPO_DIR not in sys.path:\n",
                "    sys.path.append(REPO_DIR)\n",
                "source_path = os.path.join(REPO_DIR, \"src\", \"copied_waveletDiff\", \"src\")\n",
                "if source_path not in sys.path:\n",
                "    sys.path.append(source_path)\n",
                "\n",
                "# Create Directories\n",
                "os.makedirs(os.path.dirname(OPTUNA_DB_PATH), exist_ok=True)\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "print(\"‚úÖ Setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_fabric",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Initialize Fabric\n",
                "from src.torch_gpu_waveletDiff.train import trainer\n",
                "\n",
                "# Setup Fabric\n",
                "fabric = trainer.setup_fabric(precision=\"bf16-mixed\", matmul_precision=\"high\")\n",
                "\n",
                "# Base Config\n",
                "BASE_CONFIG = {\n",
                "    'dataset': {'name': DATASET_NAME, 'seq_len': SEQ_LEN},\n",
                "    'training': {'batch_size': DEFAULT_HYPERPARAMS['batch_size'], 'epochs': 1},\n",
                "    'data': {'data_dir': 'src/copied_waveletDiff/data/stocks', 'normalize_data': False},\n",
                "    'wavelet': {'type': WAVELET_TYPE, 'levels': WAVELET_LEVELS},\n",
                "    'model': {'prediction_target': 'noise'},\n",
                "    'attention': {'use_cross_level_attention': True},\n",
                "    'noise': {'schedule': 'exponential'},\n",
                "    'sampling': {'ddim_eta': 0.0, 'ddim_steps': None},\n",
                "    'energy': {'weight': 0.0},\n",
                "    'optimizer': {'scheduler_type': 'onecycle'}\n",
                "}\n",
                "\n",
                "print(\"‚úÖ Fabric initialized\")\n",
                "print(f\"   Device: {fabric.device}\")\n",
                "print(f\"   Precision: bf16-mixed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "create_study",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Create Optuna Study\n",
                "import optuna\n",
                "from optuna.pruners import HyperbandPruner, MedianPruner, NopPruner\n",
                "from optuna.samplers import TPESampler, RandomSampler\n",
                "\n",
                "# Storage\n",
                "storage_url = f\"sqlite:///{OPTUNA_DB_PATH}\"\n",
                "print(f\"üìÅ Storage: {storage_url}\")\n",
                "\n",
                "# Sampler\n",
                "if SAMPLER_TYPE == \"tpe\":\n",
                "    sampler = TPESampler(\n",
                "        n_startup_trials=N_STARTUP_TRIALS,\n",
                "        multivariate=True,\n",
                "        group=True,\n",
                "        constant_liar=True\n",
                "    )\n",
                "    print(f\"üß† Sampler: TPE (startup trials: {N_STARTUP_TRIALS})\")\n",
                "else:\n",
                "    sampler = RandomSampler()\n",
                "    print(\"üé≤ Sampler: Random\")\n",
                "\n",
                "# Pruner\n",
                "if not ENABLE_PRUNING or PRUNER_TYPE == \"none\":\n",
                "    pruner = NopPruner()\n",
                "    print(\"‚úÇÔ∏è Pruner: Disabled\")\n",
                "elif PRUNER_TYPE == \"hyperband\":\n",
                "    pruner = HyperbandPruner(\n",
                "        min_resource=PRUNER_MIN_RESOURCE,\n",
                "        reduction_factor=PRUNER_REDUCTION_FACTOR\n",
                "    )\n",
                "    print(f\"‚úÇÔ∏è Pruner: Hyperband (min_resource: {PRUNER_MIN_RESOURCE}, reduction: {PRUNER_REDUCTION_FACTOR})\")\n",
                "elif PRUNER_TYPE == \"median\":\n",
                "    pruner = MedianPruner(\n",
                "        n_startup_trials=5,\n",
                "        n_warmup_steps=500\n",
                "    )\n",
                "    print(\"‚úÇÔ∏è Pruner: Median\")\n",
                "\n",
                "# Create or load study\n",
                "if USE_MULTI_OBJECTIVE:\n",
                "    study = optuna.create_study(\n",
                "        study_name=STUDY_NAME,\n",
                "        storage=storage_url,\n",
                "        directions=[\"minimize\", \"minimize\", \"minimize\"],\n",
                "        sampler=sampler,\n",
                "        pruner=pruner,\n",
                "        load_if_exists=True\n",
                "    )\n",
                "    print(\"üéØ Mode: Multi-objective (Pareto optimization)\")\n",
                "    print(\"   Objectives: [loss, step_time_ms, grad_norm_variance]\")\n",
                "else:\n",
                "    study = optuna.create_study(\n",
                "        study_name=STUDY_NAME,\n",
                "        storage=storage_url,\n",
                "        direction=\"minimize\",\n",
                "        sampler=sampler,\n",
                "        pruner=pruner,\n",
                "        load_if_exists=True\n",
                "    )\n",
                "    print(\"üéØ Mode: Single-objective (weighted scalarization)\")\n",
                "    print(f\"   Weights: loss={WEIGHT_LOSS}, speed={WEIGHT_SPEED}, stability={WEIGHT_STABILITY}\")\n",
                "\n",
                "print(f\"\\nüìä Study: {STUDY_NAME}\")\n",
                "print(f\"   Previous trials: {len(study.trials)}\")\n",
                "print(f\"   Completed: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
                "print(f\"   Pruned: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b74da977",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Configure Ngrok Auth Token\n",
                "\n",
                "# Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
                "NGROK_AUTH_TOKEN = \"37jaSY6tfwZBkmLywRS4UkVdAfY_aowS32CceyAoxg9VxvfW\"  # @param {type:\"string\"}\n",
                "\n",
                "if NGROK_AUTH_TOKEN:\n",
                "    from pyngrok import ngrok\n",
                "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
                "    print(\"‚úÖ Ngrok authenticated\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Please set your ngrok auth token\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dashboard",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 5: Launch Optuna Dashboard (Optional)\n",
                "if ENABLE_DASHBOARD:\n",
                "    import subprocess\n",
                "    import time\n",
                "    \n",
                "    # Kill any existing dashboard\n",
                "    !pkill -f \"optuna-dashboard\"\n",
                "    \n",
                "    # Start dashboard in background\n",
                "    dashboard_process = subprocess.Popen(\n",
                "        [\"optuna-dashboard\", storage_url, \"--port\", str(DASHBOARD_PORT)],\n",
                "        stdout=subprocess.PIPE,\n",
                "        stderr=subprocess.PIPE\n",
                "    )\n",
                "    \n",
                "    time.sleep(3)\n",
                "    \n",
                "    # Create ngrok tunnel\n",
                "    try:\n",
                "        from pyngrok import ngrok\n",
                "        public_url = ngrok.connect(DASHBOARD_PORT)\n",
                "        \n",
                "        print(\"=\"*60)\n",
                "        print(\"üé® OPTUNA DASHBOARD READY\")\n",
                "        print(\"=\"*60)\n",
                "        print(f\"üåê Public URL: {public_url}\")\n",
                "        print(f\"üìä Study: {STUDY_NAME}\")\n",
                "        print(\"=\"*60)\n",
                "        print(\"\\n‚ö†Ô∏è Keep this cell running! Dashboard will stop if interrupted.\")\n",
                "    except ImportError:\n",
                "        print(\"Installing pyngrok...\")\n",
                "        !pip install -q pyngrok\n",
                "        print(\"Please re-run this cell after installation.\")\n",
                "else:\n",
                "    print(\"üìä Dashboard disabled. Set ENABLE_DASHBOARD=True to enable.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "optimize",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 6: Run Hyperparameter Optimization\n",
                "from src.torch_gpu_waveletDiff.train.optuna_trainer import OptunaWaveletDiffTrainer\n",
                "\n",
                "# Create Optuna trainer\n",
                "optuna_trainer = OptunaWaveletDiffTrainer(\n",
                "    fabric=fabric,\n",
                "    config_base=BASE_CONFIG,\n",
                "    repo_dir=REPO_DIR,\n",
                "    data_path=DATA_PATH,\n",
                "    tune_flags=TUNE_FLAGS,\n",
                "    default_hyperparams=DEFAULT_HYPERPARAMS,\n",
                "    checkpoint_dir=CHECKPOINT_DIR,\n",
                "    trial_steps=STEPS_PER_TRIAL,\n",
                "    eval_interval=EVAL_INTERVAL\n",
                ")\n",
                "\n",
                "# Select objective function\n",
                "if USE_MULTI_OBJECTIVE:\n",
                "    objective_fn = optuna_trainer.objective\n",
                "else:\n",
                "    objective_fn = optuna_trainer.objective_single\n",
                "\n",
                "# Run optimization\n",
                "print(\"=\"*60)\n",
                "print(\"üöÄ STARTING OPTIMIZATION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Trials: {N_TRIALS}\")\n",
                "print(f\"Steps per trial: {STEPS_PER_TRIAL}\")\n",
                "print(f\"Timeout: {TIMEOUT_HOURS or 'None'} hours\")\n",
                "print(f\"Mode: {'Multi-objective' if USE_MULTI_OBJECTIVE else 'Single-objective'}\")\n",
                "print(f\"Tuning {len([v for v in TUNE_FLAGS.values() if v])} hyperparameters\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "try:\n",
                "    study.optimize(\n",
                "        objective_fn,\n",
                "        n_trials=N_TRIALS,\n",
                "        timeout=TIMEOUT_HOURS * 3600 if TIMEOUT_HOURS else None,\n",
                "        n_jobs=1,\n",
                "        show_progress_bar=True\n",
                "    )\n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\n‚ö†Ô∏è Optimization interrupted by user\")\n",
                "\n",
                "print(\"\\n‚úÖ Optimization complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 7: Analyze Results\n",
                "import optuna\n",
                "from optuna.visualization import (\n",
                "    plot_optimization_history,\n",
                "    plot_param_importances,\n",
                "    plot_parallel_coordinate,\n",
                "    plot_pareto_front\n",
                ")\n",
                "\n",
                "# Reload study\n",
                "study = optuna.load_study(\n",
                "    study_name=STUDY_NAME,\n",
                "    storage=f\"sqlite:///{OPTUNA_DB_PATH}\"\n",
                ")\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üìä OPTIMIZATION RESULTS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Total trials: {len(study.trials)}\")\n",
                "print(f\"Completed: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
                "print(f\"Pruned: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
                "print(f\"Failed: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
                "\n",
                "if USE_MULTI_OBJECTIVE:\n",
                "    print(\"\\nüéØ Top Pareto-Optimal Trials:\")\n",
                "    pareto_trials = study.best_trials[:5]\n",
                "    for i, trial in enumerate(pareto_trials):\n",
                "        print(f\"\\nTrial {trial.number}:\")\n",
                "        print(f\"  Loss: {trial.values[0]:.6f}\")\n",
                "        print(f\"  Step Time: {trial.values[1]:.2f}ms\")\n",
                "        print(f\"  Grad Variance: {trial.values[2]:.6f}\")\n",
                "        print(f\"  Key Params: embed_dim={trial.params.get('embed_dim', 'N/A')}, \"\n",
                "              f\"layers={trial.params.get('num_layers', 'N/A')}, \"\n",
                "              f\"batch={trial.params.get('batch_size', 'N/A')}\")\n",
                "else:\n",
                "    print(f\"\\nüèÜ Best Trial: {study.best_trial.number}\")\n",
                "    print(f\"   Best Value: {study.best_value:.6f}\")\n",
                "    print(f\"   Best Params:\")\n",
                "    for key, value in study.best_params.items():\n",
                "        print(f\"      {key}: {value}\")\n",
                "\n",
                "# Visualizations\n",
                "print(\"\\nüìà Generating visualizations...\")\n",
                "\n",
                "try:\n",
                "    fig1 = plot_optimization_history(study)\n",
                "    fig1.show()\n",
                "except:\n",
                "    print(\"Could not plot optimization history\")\n",
                "\n",
                "try:\n",
                "    if len(study.trials) > 10:\n",
                "        fig2 = plot_param_importances(study)\n",
                "        fig2.show()\n",
                "except:\n",
                "    print(\"Could not plot parameter importances\")\n",
                "\n",
                "try:\n",
                "    fig3 = plot_parallel_coordinate(study)\n",
                "    fig3.show()\n",
                "except:\n",
                "    print(\"Could not plot parallel coordinates\")\n",
                "\n",
                "if USE_MULTI_OBJECTIVE:\n",
                "    try:\n",
                "        fig4 = plot_pareto_front(study)\n",
                "        fig4.show()\n",
                "    except:\n",
                "        print(\"Could not plot Pareto front\")\n",
                "\n",
                "print(\"\\n‚úÖ Analysis complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 8: Export Best Configurations\n",
                "import json\n",
                "\n",
                "if USE_MULTI_OBJECTIVE:\n",
                "    print(\"üéØ Exporting top 3 Pareto-optimal configurations:\\n\")\n",
                "    best_trials = study.best_trials[:3]\n",
                "else:\n",
                "    print(\"üèÜ Exporting best configuration:\\n\")\n",
                "    best_trials = [study.best_trial]\n",
                "\n",
                "for i, trial in enumerate(best_trials):\n",
                "    config_export = {\n",
                "        \"trial_number\": trial.number,\n",
                "        \"hyperparameters\": trial.params,\n",
                "        \"user_attrs\": dict(trial.user_attrs),\n",
                "        \"state\": str(trial.state)\n",
                "    }\n",
                "    \n",
                "    if USE_MULTI_OBJECTIVE:\n",
                "        config_export[\"objectives\"] = {\n",
                "            \"loss\": trial.values[0],\n",
                "            \"step_time_ms\": trial.values[1],\n",
                "            \"grad_variance\": trial.values[2]\n",
                "        }\n",
                "    else:\n",
                "        config_export[\"objective_value\"] = trial.value\n",
                "    \n",
                "    # Save to file\n",
                "    filename = f\"{CHECKPOINT_DIR}/best_config_trial_{trial.number}.json\"\n",
                "    with open(filename, 'w') as f:\n",
                "        json.dump(config_export, f, indent=2)\n",
                "    \n",
                "    print(f\"Trial {trial.number}:\")\n",
                "    print(json.dumps(config_export, indent=2))\n",
                "    print(f\"\\nüíæ Saved to: {filename}\\n\")\n",
                "    print(\"-\"*60)\n",
                "\n",
                "print(\"\\n‚úÖ Configurations exported\")\n",
                "print(f\"\\nTo use these hyperparameters, update your training notebook with values from above.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WaveletDiff TPU Training (Keras 3 / JAX)\n",
                "\n",
                "This notebook implements the high-performance training pipeline for WaveletDiff using Keras 3 with JAX backend, specifically optimized for Cloud TPUs (v5e/v6e).\n",
                "\n",
                "### Features:\n",
                "- **JAX Backend**: Uses XLA compilation for maximum throughput.\n",
                "- **tf.data Pipeline**: Asynchronous prefetching via `tf.data` (avoids Python threading bottlenecks).\n",
                "- **Fused Steps**: The entire training step (sampling + diff + loss) is valid XLA graph.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a85ada20",
            "metadata": {
                "id": "setup_cell"
            },
            "outputs": [],
            "source": [
                "# @title Cell 1: Environment Setup & Cloning\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# 1. Set Backend to JAX (Must be done before importing keras)\n",
                "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
                "\n",
                "# 2. Clone Repository\n",
                "REPO_DIR = \"/content/waveletDiff_synth_data\"\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    !git clone https://github.com/MilesHoffman/waveletDiff_synth_data.git {REPO_DIR}\n",
                "else:\n",
                "    print(\"Repo already exists. Pulling latest...\")\n",
                "    !cd {REPO_DIR} && git pull\n",
                "\n",
                "# 3. Dependencies\n",
                "!pip install keras --upgrade  # Ensure Keras 3\n",
                "!pip install pywavelets\n",
                "\n",
                "# 4. Path Setup\n",
                "if REPO_DIR not in sys.path:\n",
                "    sys.path.append(f\"{REPO_DIR}/src\")\n",
                "    print(f\"Added {REPO_DIR}/src to path\")\n",
                "\n",
                "import keras\n",
                "import jax\n",
                "\n",
                "print(f\"Keras version: {keras.__version__}\")\n",
                "print(f\"Backend: {keras.config.backend()}\")\n",
                "\n",
                "# --- Hardware Optimization ---\n",
                "try:\n",
                "    devices = jax.devices()\n",
                "    device_type = devices[0].platform.upper()\n",
                "    print(f\"Hardware Detected: {device_type} (Count: {len(devices)})\")\n",
                "    \n",
                "    if device_type == 'TPU':\n",
                "        keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
                "        print(\"✅ Optimization: Precision set to 'mixed_bfloat16' (TPU Native)\")\n",
                "    elif device_type == 'GPU':\n",
                "        keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
                "        print(\"✅ Optimization: Precision set to 'mixed_float16'\")\n",
                "except Exception as e:\n",
                "    print(f\"Hardware detection failed: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Configuration\n",
                "\n",
                "# Dataset\n",
                "DATA_PATH = f\"{REPO_DIR}/src/copied_waveletDiff/data/stocks/stock_data.csv\"\n",
                "SEQ_LEN = 24\n",
                "BATCH_SIZE = 4096 # Large batch for TPU efficiency\n",
                "\n",
                "# Model\n",
                "EMBED_DIM = 256\n",
                "NUM_HEADS = 8\n",
                "NUM_LAYERS = 8\n",
                "DROPOUT = 0.1\n",
                "LEARNING_RATE = 2e-4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4dd8d00e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Load Data & Initialize Model\n",
                "from w_keras import data as kdata\n",
                "from w_keras import transformer as ktrans\n",
                "import keras\n",
                "\n",
                "# 1. Load Data (tf.data Pipeline)\n",
                "# Note: The first time this runs, it will process the CSV into wavelets.\n",
                "ds, info = kdata.load_dataset(DATA_PATH, BATCH_SIZE, SEQ_LEN)\n",
                "\n",
                "print(f\"Wavelet Info: {info['level_dims']} coeffs per level\")\n",
                "\n",
                "# 2. Initialize Model\n",
                "model_config = {\n",
                "    'embed_dim': EMBED_DIM,\n",
                "    'num_heads': NUM_HEADS,\n",
                "    'num_layers': NUM_LAYERS,\n",
                "    'dropout': DROPOUT,\n",
                "    'prediction_target': 'noise'\n",
                "}\n",
                "\n",
                "model = ktrans.WaveletDiffusionTransformer(info, model_config)\n",
                "\n",
                "# 3. Compile (XLA Just-In-Time)\n",
                "optimizer = keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-5)\n",
                "model.compile(optimizer=optimizer, jit_compile=True)\n",
                "\n",
                "# Dummy build to print summary\n",
                "# Keras models are built lazily, so we pass one batch to shape it\n",
                "for batch in ds.take(1):\n",
                "    model.predict(batch[0], verbose=0)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Train\n",
                "history = model.fit(\n",
                "    ds,\n",
                "    epochs=50,\n",
                "    steps_per_epoch=200 # Adjust based on dataset size\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "config"
            },
            "outputs": [],
            "source": [
                "# @title Global Configuration\n",
                "# Hyperparameters (Matched to configs/datasets/stocks.yaml)\n",
                "BATCH_SIZE = 512\n",
                "LEARNING_RATE = 2e-4\n",
                "EPOCHS = 5000\n",
                "NUM_STEPS = 1000  # Diffusion timesteps\n",
                "MODEL_DIM = 256\n",
                "NUM_LAYERS = 8\n",
                "\n",
                "# Data Configuration\n",
                "DATASET_NAME = \"stocks\"\n",
                "SEQ_LEN = 24\n",
                "WAVELET_TYPE = \"db2\"\n",
                "# Note: NUM_WAVELET_LEVELS will be determined automatically by the DataBridge\n",
                "TRAIN_DATA_PATH = \"src/data/stocks/stock_data.csv\"\n",
                "\n",
                "# Path Configuration\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data\"\n",
                "PROJECT_ROOT = \"/content/waveletDiff_synth_data\"\n",
                "DATA_DIR = \"/content/data\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "# @title Imports & Environment Setup\n",
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "\n",
                "# 1. Setup Keras/JAX Backend\n",
                "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
                "\n",
                "# 2. Repository Management\n",
                "if not os.path.exists(PROJECT_ROOT):\n",
                "    !git clone {REPO_URL}\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")\n",
                "\n",
                "# CRITICAL: Unify namespace by injecting missing modules into the PROJECT'S 'src' package\n",
                "# This resolves conflicting 'src' namespaces and ensures 'src.data.module' is available.\n",
                "\n",
                "source_repo_path = os.path.join(PROJECT_ROOT, \"waveletDiff_source_repo\")\n",
                "project_src_path = os.path.join(PROJECT_ROOT, \"src\")\n",
                "\n",
                "# 1. Inject Data Modules (module.py, loaders.py, __init__.py)\n",
                "# We copy from waveletDiff_source_repo/src/data to src/data\n",
                "source_data_dir = os.path.join(source_repo_path, \"src\", \"data\")\n",
                "target_data_dir = os.path.join(project_src_path, \"data\")\n",
                "\n",
                "if os.path.exists(source_data_dir) and os.path.exists(target_data_dir):\n",
                "    for filename in os.listdir(source_data_dir):\n",
                "        src_file = os.path.join(source_data_dir, filename)\n",
                "        dst_file = os.path.join(target_data_dir, filename)\n",
                "        # Copy only if it's a file and doesn't exist (or just overwrite to be safe)\n",
                "        if os.path.isfile(src_file):\n",
                "            shutil.copy2(src_file, dst_file)\n",
                "            print(f\"Injected {filename} into {target_data_dir}\")\n",
                "\n",
                "# 2. Inject Utils Package\n",
                "# Copy waveletDiff_source_repo/src/utils to src/utils\n",
                "source_utils_dir = os.path.join(source_repo_path, \"src\", \"utils\")\n",
                "target_utils_dir = os.path.join(project_src_path, \"utils\")\n",
                "\n",
                "if os.path.exists(source_utils_dir):\n",
                "    if os.path.exists(target_utils_dir):\n",
                "        shutil.rmtree(target_utils_dir) # Clean replace to ensure consistency\n",
                "    shutil.copytree(source_utils_dir, target_utils_dir)\n",
                "    print(f\"Injected utils package at {target_utils_dir}\")\n",
                "\n",
                "# 3. Inject Custom Training Data\n",
                "# Verify DATA_DIR exists\n",
                "if not os.path.exists(DATA_DIR):\n",
                "    os.makedirs(DATA_DIR)\n",
                "\n",
                "if TRAIN_DATA_PATH:\n",
                "    source_path = os.path.join(PROJECT_ROOT, TRAIN_DATA_PATH)\n",
                "    target_dir = os.path.join(DATA_DIR, \"stocks\")\n",
                "    os.makedirs(target_dir, exist_ok=True)\n",
                "    \n",
                "    # Check if source exists\n",
                "    if os.path.exists(source_path):\n",
                "        shutil.copy2(source_path, os.path.join(target_dir, \"stock_data.csv\"))\n",
                "        print(f\"Injected custom training data from {source_path} to {target_dir}/stock_data.csv\")\n",
                "    else:\n",
                "        print(f\"Warning: Custom data path {source_path} not found. Using default/synthetic if available.\")\n",
                "\n",
                "# Ensure project root is in path\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.append(PROJECT_ROOT)\n",
                "\n",
                "# Remove /content from path if present to avoid finding empty src there\n",
                "if \"/content\" in sys.path:\n",
                "    try:\n",
                "        sys.path.remove(\"/content\")\n",
                "    except ValueError:\n",
                "        pass\n",
                "\n",
                "# 3. Dependency Installation\n",
                "!pip install -q keras flax optax PyWavelets numpy lightning torch\n",
                "\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import keras\n",
                "import numpy as np\n",
                "\n",
                "print(f\"Using Backend: {jax.lib.xla_bridge.get_backend().platform}\")\n",
                "print(f\"Devices: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "data_bridge"
            },
            "outputs": [],
            "source": [
                "# @title Production Data Orchestration\n",
                "from src.data.module import WaveletTimeSeriesDataModule\n",
                "from src.tpu_keras.data_bridge import JAXDataBridge\n",
                "\n",
                "# Initialize the original DataModule\n",
                "# If you have a custom csv, place it in DATA_DIR and ensuring naming matches defaults or update config\n",
                "config = {\n",
                "    'dataset': {'name': DATASET_NAME, 'seq_len': SEQ_LEN},\n",
                "    'training': {'batch_size': BATCH_SIZE},\n",
                "    'data': {'data_dir': DATA_DIR, 'normalize_data': True},\n",
                "    'wavelet': {'type': WAVELET_TYPE, 'levels': \"auto\"}\n",
                "}\n",
                "\n",
                "# Note: This might require actual data files in /content/data\n",
                "try:\n",
                "    dm = WaveletTimeSeriesDataModule(config=config)\n",
                "    bridge = JAXDataBridge(dm)\n",
                "    dataloader = bridge.get_iterator()\n",
                "    LEVEL_DIMS = bridge.get_level_dims()\n",
                "    # Dynamically determine the number of levels from the data\n",
                "    NUM_WAVELET_LEVELS = len(LEVEL_DIMS) - 1\n",
                "    print(f\"Production DataModule initialized. Detected {NUM_WAVELET_LEVELS} decomposition levels.\")\n",
                "    print(f\"Level Dimensions: {LEVEL_DIMS}\")\n",
                "except Exception as e:\n",
                "    print(f\"Warning: Could not load real data: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()\n",
                "    print(\"Falling back to synthetic data structure for initialization.\")\n",
                "    # Fallback for demo purposes if data is missing\n",
                "    NUM_WAVELET_LEVELS = 3\n",
                "    LEVEL_DIMS = [8, 8, 16, 32] # Approx for len 24? Actually db2 l3 might differ.\n",
                "    def synthetic_gen():\n",
                "        while True:\n",
                "            yield [np.random.randn(BATCH_SIZE, d, 1).astype('float32') for d in LEVEL_DIMS]\n",
                "    dataloader = synthetic_gen()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_init"
            },
            "outputs": [],
            "source": [
                "# @title WaveletDiff TPU Backend Optimization\n",
                "from src.tpu_keras.models.transformer import WaveletDiffusionTransformer\n",
                "from src.tpu_keras.models.diffusion import DiffusionScheduler\n",
                "from src.tpu_keras.models.losses import WaveletLoss\n",
                "from src.tpu_keras.trainer import TPUTrainer\n",
                "\n",
                "# 1. Model Assembly (with TPU-optimized architecture)\n",
                "# We match the source 'stocks.yaml' config exactly\n",
                "model = WaveletDiffusionTransformer(\n",
                "    input_dim=1,\n",
                "    model_dim=MODEL_DIM,\n",
                "    num_levels=NUM_WAVELET_LEVELS,\n",
                "    num_layers_per_level=NUM_LAYERS\n",
                ")\n",
                "\n",
                "# 2. Noise Scheduling\n",
                "# Note: Source uses 'exponential', mapped to 'cosine' here for high-quality baseline.\n",
                "scheduler = DiffusionScheduler(num_steps=NUM_STEPS, schedule_type='cosine')\n",
                "\n",
                "# 3. Wavelet-Aware Loss Function\n",
                "loss_fn = WaveletLoss(level_dims=LEVEL_DIMS, strategy=\"coefficient_weighted\")\n",
                "\n",
                "# 4. High-Throughput TPU Trainer\n",
                "trainer = TPUTrainer(\n",
                "    model=model,\n",
                "    scheduler=scheduler,\n",
                "    loss_fn=loss_fn,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    steps_per_epoch=100,\n",
                "    log_interval_percent=1  # Eliminates Host-TPU bottleneck\n",
                ")\n",
                "\n",
                "print(\"Backend modules fully integrated and ready for TPU training.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_loop"
            },
            "outputs": [],
            "source": [
                "# @title Training Loop Execution\n",
                "for epoch in range(1, EPOCHS + 1):\n",
                "    # Each epoch executes multi-step logic on device without host interruption\n",
                "    trainer.train_epoch(dataloader, epoch)\n",
                "    \n",
                "    # Periodic Sampling for Quality Monitoring\n",
                "    if epoch % 100 == 0:\n",
                "        print(f\" Landmark reached at Epoch {epoch} - Reviewing sample distribution...\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
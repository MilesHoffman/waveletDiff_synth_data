{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "config"
            },
            "outputs": [],
            "source": [
                "# @title Global Configuration\n",
                "# Hyperparameters\n",
                "BATCH_SIZE = 128\n",
                "LEARNING_RATE = 2e-4\n",
                "EPOCHS = 1000\n",
                "NUM_STEPS = 1000  # Diffusion timesteps\n",
                "MODEL_DIM = 256\n",
                "NUM_LAYERS = 2\n",
                "\n",
                "# Data Configuration\n",
                "DATASET_NAME = \"stocks\"  # stocks, fmri, ett, eeg\n",
                "SEQ_LEN = 128\n",
                "WAVELET_TYPE = \"db4\"\n",
                "NUM_WAVELET_LEVELS = 3\n",
                "\n",
                "# Path Configuration\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data\"\n",
                "PROJECT_ROOT = \"/content/waveletDiff_synth_data\"\n",
                "DATA_DIR = \"/content/data\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "# @title Imports & Environment Setup\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# 1. Setup Keras/JAX Backend\n",
                "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
                "\n",
                "# 2. Repository Management\n",
                "if not os.path.exists(PROJECT_ROOT):\n",
                "    !git clone {REPO_URL}\n",
                "\n",
                "# Ensure project root and src are in path\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.append(PROJECT_ROOT)\n",
                "\n",
                "# 3. Dependency Installation\n",
                "!pip install -q keras-core flax optax pywt numpy lightning torch\n",
                "\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import keras_core as keras\n",
                "import numpy as np\n",
                "\n",
                "print(f\"Using Backend: {jax.lib.xla_bridge.get_backend().platform}\")\n",
                "print(f\"Devices: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "data_bridge"
            },
            "outputs": [],
            "source": [
                "# @title Production Data Orchestration\n",
                "from src.data.module import WaveletTimeSeriesDataModule\n",
                "from src.tpu_keras.data_bridge import JAXDataBridge\n",
                "\n",
                "# Initialize the original DataModule\n",
                "config = {\n",
                "    'dataset': {'name': DATASET_NAME, 'seq_len': SEQ_LEN},\n",
                "    'training': {'batch_size': BATCH_SIZE},\n",
                "    'data': {'data_dir': DATA_DIR, 'normalize_data': True},\n",
                "    'wavelet': {'type': WAVELET_TYPE, 'levels': NUM_WAVELET_LEVELS}\n",
                "}\n",
                "\n",
                "# Note: This might require actual data files in /content/data\n",
                "try:\n",
                "    dm = WaveletTimeSeriesDataModule(config=config)\n",
                "    bridge = JAXDataBridge(dm)\n",
                "    dataloader = bridge.get_iterator()\n",
                "    LEVEL_DIMS = bridge.get_level_dims()\n",
                "    print(\"Production DataModule and JAX Bridge initialized successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Warning: Could not load real data: {e}\")\n",
                "    print(\"Falling back to synthetic data structure for initialization.\")\n",
                "    LEVEL_DIMS = [16, 16, 32, 64] #db4, level 3, seq 128\n",
                "    def synthetic_gen():\n",
                "        while True:\n",
                "            yield [np.random.randn(BATCH_SIZE, d, 1).astype('float32') for d in LEVEL_DIMS]\n",
                "    dataloader = synthetic_gen()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_init"
            },
            "outputs": [],
            "source": [
                "# @title WaveletDiff TPU Backend Optimization\n",
                "from src.tpu_keras.models.transformer import WaveletDiffusionTransformer\n",
                "from src.tpu_keras.models.diffusion import DiffusionScheduler\n",
                "from src.tpu_keras.models.losses import WaveletLoss\n",
                "from src.tpu_keras.trainer import TPUTrainer\n",
                "\n",
                "# 1. Model Assembly (with TPU-optimized architecture)\n",
                "model = WaveletDiffusionTransformer(\n",
                "    input_dim=1,\n",
                "    model_dim=MODEL_DIM,\n",
                "    num_levels=NUM_WAVELET_LEVELS,\n",
                "    num_layers_per_level=NUM_LAYERS\n",
                ")\n",
                "\n",
                "# 2. Noise Scheduling (Cosine for better gradients)\n",
                "scheduler = DiffusionScheduler(num_steps=NUM_STEPS, schedule_type='cosine')\n",
                "\n",
                "# 3. Wavelet-Aware Loss Function\n",
                "loss_fn = WaveletLoss(level_dims=LEVEL_DIMS, strategy=\"coefficient_weighted\")\n",
                "\n",
                "# 4. High-Throughput TPU Trainer\n",
                "trainer = TPUTrainer(\n",
                "    model=model,\n",
                "    scheduler=scheduler,\n",
                "    loss_fn=loss_fn,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    steps_per_epoch=100,\n",
                "    log_interval_percent=1  # Eliminates Host-TPU bottleneck\n",
                ")\n",
                "\n",
                "print(\"Backend modules fully integrated and ready for TPU training.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_loop"
            },
            "outputs": [],
            "source": [
                "# @title Training Loop Execution\n",
                "for epoch in range(1, EPOCHS + 1):\n",
                "    # Each epoch executes multi-step logic on device without host interruption\n",
                "    trainer.train_epoch(dataloader, epoch)\n",
                "    \n",
                "    # Periodic Sampling for Quality Monitoring\n",
                "    if epoch % 100 == 0:\n",
                "        print(f\" Landmark reached at Epoch {epoch} - Reviewing sample distribution...\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
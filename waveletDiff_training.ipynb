{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "639c738b",
            "metadata": {},
            "source": [
                "# WaveletDiff Training (Stocks Dataset)\n",
                "\n",
                "This notebook trains the WaveletDiff model on the **stocks** dataset using the modular `src` directory.\n",
                "\n",
                "### Workflow:\n",
                "1. **Configuration**: Tune hyperparameters, paths, and precision.\n",
                "2. **Setup**: Clones the repo, installs dependencies.\n",
                "3. **Cache**: Restore torch.compile cache if available.\n",
                "4. **Environment**: Configures PyTorch precision and seeds.\n",
                "5. **Data**: Load and prepare data.\n",
                "6. **Model**: Initialize and compile the model.\n",
                "7. **Train**: Run training.\n",
                "8. **Cache**: Save torch.compile cache to Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5be5e355",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 1: Global Configuration\n",
                "import random\n",
                "import os\n",
                "\n",
                "# --- Experiment Identity ---\n",
                "RUN_NAME = \"stocks_baseline_v1\" # @param {type:\"string\"}\n",
                "RUN_ID = f\"{random.randint(0, 999):03d}\"\n",
                "UNIQUE_RUN_NAME = f\"{RUN_NAME}_{RUN_ID}\"\n",
                "\n",
                "# --- Dataset (Stocks-specific, fixed) ---\n",
                "DATASET_NAME = \"stocks\"\n",
                "SEQ_LEN = 24\n",
                "NORMALIZE_DATA = True\n",
                "\n",
                "# --- Model Hyperparameters ---\n",
                "EMBED_DIM = 256 # @param {type:\"integer\"}\n",
                "NUM_HEADS = 8 # @param {type:\"integer\"}\n",
                "NUM_LAYERS = 8 # @param {type:\"integer\"}\n",
                "TIME_EMBED_DIM = 128 # @param {type:\"integer\"}\n",
                "DROPOUT = 0.1 # @param {type:\"number\"}\n",
                "\n",
                "# --- Training Hyperparameters ---\n",
                "NUM_EPOCHS = 5000 # @param {type:\"integer\"}\n",
                "BATCH_SIZE = 512 # @param {type:\"integer\"}\n",
                "LEARNING_RATE = 2e-4 # @param {type:\"number\"}\n",
                "GRADIENT_CLIP_VAL = 1.0 # @param {type:\"number\"}\n",
                "\n",
                "# --- Optimizer Hyperparameters ---\n",
                "WEIGHT_DECAY = 1e-5 # @param {type:\"number\"}\n",
                "ONECYCLE_MAX_LR = 1e-3 # @param {type:\"number\"}\n",
                "ONECYCLE_PCT_START = 0.3 # @param {type:\"number\"}\n",
                "\n",
                "# --- Logging & Checkpointing ---\n",
                "LOG_EVERY_N_EPOCHS = 1 # @param {type:\"integer\"}\n",
                "LOG_LEVEL_LOSSES_EVERY_N_EPOCHS = 100 # @param {type:\"integer\"}\n",
                "LOG_EVERY_N_STEPS = 50 # @param {type:\"integer\"}\n",
                "SAVE_EVERY_N_EPOCHS = 1000 # @param {type:\"integer\"}\n",
                "\n",
                "# --- Hardware & Precision ---\n",
                "PRECISION = \"32\" # @param [\"32\", \"bf16-mixed\"]\n",
                "MATMUL_PRECISION = \"medium\" # @param [\"medium\", \"high\"]\n",
                "\n",
                "# --- torch.compile Settings ---\n",
                "COMPILE_MODE = None # @param [None, \"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"]\n",
                "COMPILE_CACHE_DIR = \"/content/torchinductor_cache\" # @param {type:\"string\"}\n",
                "\n",
                "# --- Paths ---\n",
                "DRIVE_BASE_PATH = \"/content/drive/MyDrive/personal_drive/trading/waveletDiff\" # @param {type:\"string\"}\n",
                "OUTPUT_DIR = f\"{DRIVE_BASE_PATH}/checkpoints/temp/{UNIQUE_RUN_NAME}\"\n",
                "COMPILE_CACHE_DRIVE_PATH = f\"{DRIVE_BASE_PATH}/compile_cache\" # @param {type:\"string\"}\n",
                "REPO_URL = \"https://github.com/MilesHoffman/waveletDiff_synth_data.git\"\n",
                "REPO_DIR = \"/content/waveletDiff_synth_data\"\n",
                "\n",
                "# Specific path to your CSV file\n",
                "DATA_PATH = \"/content/waveletDiff_synth_data/stocks/stock_data.csv\" # @param {type:\"string\"}\n",
                "\n",
                "# --- Reproducibility ---\n",
                "SEED = 42 # @param {type:\"integer\"}\n",
                "\n",
                "# --- Fixed Settings (Stocks configuration) ---\n",
                "PREDICTION_TARGET = \"noise\"\n",
                "USE_CROSS_LEVEL_ATTENTION = True\n",
                "ENERGY_WEIGHT = 0.0\n",
                "NOISE_SCHEDULE = \"exponential\"\n",
                "SCHEDULER_TYPE = \"onecycle\"\n",
                "WAVELET_TYPE = \"db2\"\n",
                "WAVELET_LEVELS = \"auto\"\n",
                "DDIM_ETA = 0.0\n",
                "DDIM_STEPS = None\n",
                "ACCELERATOR = \"gpu\"\n",
                "DEVICES = 1\n",
                "SAVE_TOP_K = -1\n",
                "WARMUP_EPOCHS = 50"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d31d0907",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 2: Setup (Clone, Install, Mount)\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    if not os.path.exists('/content/drive'):\n",
                "        drive.mount('/content/drive')\n",
                "    print(\"✅ Drive mounted\")\n",
                "except ImportError:\n",
                "    print(\"Not running on Colab. Skipping Drive mount.\")\n",
                "    DRIVE_BASE_PATH = \"local_checkpoints\"\n",
                "    os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    print(f\"Cloning {REPO_URL} into {REPO_DIR}...\")\n",
                "    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
                "else:\n",
                "    print(f\"Repo exists at {REPO_DIR}, pulling latest changes...\")\n",
                "    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\"], check=True)\n",
                "\n",
                "if REPO_DIR not in sys.path:\n",
                "    sys.path.insert(0, REPO_DIR)\n",
                "\n",
                "print(\"Installing dependencies...\")\n",
                "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pytorch-lightning\", \"pywavelets\", \"scipy\", \"pandas\", \"tqdm\", \"lightning\"], check=True)\n",
                "print(\"✅ Dependencies installed\")\n",
                "\n",
                "import importlib\n",
                "importlib.invalidate_caches()\n",
                "print(\"✅ Env Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cache_restore",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 3: Restore Compile Cache\n",
                "import os\n",
                "import tarfile\n",
                "import shutil\n",
                "\n",
                "CACHE_ARCHIVE_NAME = \"torchinductor_cache.tar.gz\"\n",
                "LOCAL_CACHE_ARCHIVE = f\"/content/{CACHE_ARCHIVE_NAME}\"\n",
                "DRIVE_CACHE_ARCHIVE = f\"{COMPILE_CACHE_DRIVE_PATH}/{CACHE_ARCHIVE_NAME}\"\n",
                "\n",
                "if COMPILE_MODE is not None:\n",
                "    os.makedirs(COMPILE_CACHE_DIR, exist_ok=True)\n",
                "    os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = COMPILE_CACHE_DIR\n",
                "    \n",
                "    # Check if cache archive exists locally first\n",
                "    if os.path.exists(LOCAL_CACHE_ARCHIVE):\n",
                "        print(f\"Found local cache archive, extracting...\")\n",
                "        with tarfile.open(LOCAL_CACHE_ARCHIVE, \"r:gz\") as tar:\n",
                "            tar.extractall(COMPILE_CACHE_DIR)\n",
                "        print(f\"✅ Cache restored from local archive\")\n",
                "    elif os.path.exists(DRIVE_CACHE_ARCHIVE):\n",
                "        print(f\"Found cache archive on Drive, copying and extracting...\")\n",
                "        shutil.copy(DRIVE_CACHE_ARCHIVE, LOCAL_CACHE_ARCHIVE)\n",
                "        with tarfile.open(LOCAL_CACHE_ARCHIVE, \"r:gz\") as tar:\n",
                "            tar.extractall(COMPILE_CACHE_DIR)\n",
                "        print(f\"✅ Cache restored from Drive\")\n",
                "    else:\n",
                "        print(\"No existing cache found, will compile from scratch\")\n",
                "else:\n",
                "    print(\"torch.compile disabled, skipping cache restore\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4169bed8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 4: Setup Environment\n",
                "import torch\n",
                "import pytorch_lightning as pl\n",
                "\n",
                "if SEED is not None:\n",
                "    pl.seed_everything(SEED)\n",
                "\n",
                "try:\n",
                "    torch.set_float32_matmul_precision(MATMUL_PRECISION)\n",
                "    print(f\"✅ Matmul precision set to {MATMUL_PRECISION}\")\n",
                "except Exception as e:\n",
                "    print(f\"Could not set matmul precision: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0296d61c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 5: Load Data\n",
                "import sys\n",
                "import importlib\n",
                "import numpy as np\n",
                "\n",
                "# Force reload of modules to pick up changes\n",
                "if 'src.data.loaders' in sys.modules:\n",
                "    import src.data.loaders\n",
                "    importlib.reload(src.data.loaders)\n",
                "if 'src.data.module' in sys.modules:\n",
                "    import src.data.module\n",
                "    importlib.reload(src.data.module)\n",
                "if 'src.data' in sys.modules:\n",
                "    import src.data\n",
                "    importlib.reload(src.data)\n",
                "\n",
                "from src.data import WaveletTimeSeriesDataModule\n",
                "\n",
                "config = {\n",
                "    'training': {\n",
                "        'epochs': NUM_EPOCHS,\n",
                "        'batch_size': BATCH_SIZE,\n",
                "        'save_model': True,\n",
                "        'log_every_n_epochs': LOG_EVERY_N_EPOCHS,\n",
                "        'log_level_losses_every_n_epochs': LOG_LEVEL_LOSSES_EVERY_N_EPOCHS\n",
                "    },\n",
                "    'dataset': {'name': DATASET_NAME, 'seq_len': SEQ_LEN},\n",
                "    'data': {'data_dir': REPO_DIR, 'normalize_data': NORMALIZE_DATA, 'data_path': DATA_PATH},\n",
                "    'wavelet': {'type': WAVELET_TYPE, 'levels': WAVELET_LEVELS},\n",
                "    'model': {\n",
                "        'embed_dim': EMBED_DIM, 'num_heads': NUM_HEADS, 'num_layers': NUM_LAYERS,\n",
                "        'time_embed_dim': TIME_EMBED_DIM, 'dropout': DROPOUT, 'prediction_target': PREDICTION_TARGET,\n",
                "    },\n",
                "    'attention': {'use_cross_level_attention': USE_CROSS_LEVEL_ATTENTION},\n",
                "    'energy': {'weight': ENERGY_WEIGHT},\n",
                "    'noise': {'schedule': NOISE_SCHEDULE},\n",
                "    'optimizer': {\n",
                "        'scheduler_type': SCHEDULER_TYPE, 'warmup_epochs': WARMUP_EPOCHS, 'lr': LEARNING_RATE,\n",
                "        'weight_decay': WEIGHT_DECAY, 'onecycle_max_lr': ONECYCLE_MAX_LR, 'onecycle_pct_start': ONECYCLE_PCT_START,\n",
                "    },\n",
                "    'sampling': {'ddim_eta': DDIM_ETA, 'ddim_steps': DDIM_STEPS},\n",
                "    'paths': {'output_dir': OUTPUT_DIR},\n",
                "}\n",
                "\n",
                "datamodule = WaveletTimeSeriesDataModule(config=config)\n",
                "print(f\"✅ Data loaded: {datamodule.raw_data_tensor.shape}\")\n",
                "print(f\"✅ Wavelet dimension: {datamodule.get_input_dim()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "306e5338",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 6: Initialize Model\n",
                "from src.models import WaveletDiffusionTransformer\n",
                "\n",
                "model = WaveletDiffusionTransformer(data_module=datamodule, config=config)\n",
                "\n",
                "if COMPILE_MODE is not None:\n",
                "    print(f\"Compiling model with mode='{COMPILE_MODE}'...\")\n",
                "    model = torch.compile(model, mode=COMPILE_MODE)\n",
                "    print(\"✅ Model compiled\")\n",
                "else:\n",
                "    print(\"✅ Model initialized (no compilation)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b6924e91",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 7: Run Training\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint, Timer\n",
                "import os\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "callbacks = [\n",
                "    Timer(),\n",
                "    ModelCheckpoint(\n",
                "        dirpath=OUTPUT_DIR,\n",
                "        filename='checkpoint-{epoch:02d}',\n",
                "        save_top_k=SAVE_TOP_K,\n",
                "        every_n_epochs=SAVE_EVERY_N_EPOCHS\n",
                "    )\n",
                "]\n",
                "\n",
                "trainer = pl.Trainer(\n",
                "    max_epochs=NUM_EPOCHS,\n",
                "    accelerator=ACCELERATOR,\n",
                "    devices=DEVICES,\n",
                "    strategy=\"ddp_find_unused_parameters_true\",\n",
                "    precision=PRECISION,\n",
                "    gradient_clip_val=GRADIENT_CLIP_VAL,\n",
                "    gradient_clip_algorithm=\"norm\",\n",
                "    callbacks=callbacks,\n",
                "    enable_checkpointing=True,\n",
                "    logger=False,\n",
                "    log_every_n_steps=LOG_EVERY_N_STEPS,\n",
                "    enable_progress_bar=False\n",
                ")\n",
                "\n",
                "print(\"Starting training (Progress bar disabled to match source output style)...\")\n",
                "trainer.fit(model, datamodule)\n",
                "print(f\"✅ Training finished. Checkpoints saved to {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cache_save",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Cell 8: Save Compile Cache to Drive\n",
                "import os\n",
                "import tarfile\n",
                "import shutil\n",
                "\n",
                "if COMPILE_MODE is not None:\n",
                "    CACHE_ARCHIVE_NAME = \"torchinductor_cache.tar.gz\"\n",
                "    LOCAL_CACHE_ARCHIVE = f\"/content/{CACHE_ARCHIVE_NAME}\"\n",
                "    DRIVE_CACHE_ARCHIVE = f\"{COMPILE_CACHE_DRIVE_PATH}/{CACHE_ARCHIVE_NAME}\"\n",
                "    \n",
                "    if os.path.exists(COMPILE_CACHE_DIR) and os.listdir(COMPILE_CACHE_DIR):\n",
                "        print(\"Packaging compile cache...\")\n",
                "        \n",
                "        # Create tar.gz archive\n",
                "        with tarfile.open(LOCAL_CACHE_ARCHIVE, \"w:gz\") as tar:\n",
                "            for item in os.listdir(COMPILE_CACHE_DIR):\n",
                "                item_path = os.path.join(COMPILE_CACHE_DIR, item)\n",
                "                tar.add(item_path, arcname=item)\n",
                "        \n",
                "        # Copy to Drive\n",
                "        os.makedirs(COMPILE_CACHE_DRIVE_PATH, exist_ok=True)\n",
                "        shutil.copy(LOCAL_CACHE_ARCHIVE, DRIVE_CACHE_ARCHIVE)\n",
                "        \n",
                "        archive_size_mb = os.path.getsize(LOCAL_CACHE_ARCHIVE) / (1024 * 1024)\n",
                "        print(f\"✅ Cache saved to Drive ({archive_size_mb:.1f} MB)\")\n",
                "    else:\n",
                "        print(\"No cache files to save\")\n",
                "else:\n",
                "    print(\"torch.compile disabled, no cache to save\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}